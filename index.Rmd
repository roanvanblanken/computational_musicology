---
title: "Computational Musicology - Final Portfolio"
author: "Roan van Blanken"
output: 
  flexdashboard::flex_dashboard:
    orientation: columns
    self_contained: false
---

```{r setup, include=FALSE}
library(flexdashboard)
library(spotifyr)
library(purrr)
library(dplyr)
library(ggplot2)
library(plotly)
library(tidyverse)
library(compmus)

# Get playlists
songs_i_like <- get_playlist_audio_features("", "6pl0C7qbIl5uoY3Tdf82oa")

songs_i_like <- songs_i_like %>%
  mutate(mode = ifelse(mode == 0, "Minor", "Major"))

songs_i_dislike <- get_playlist_audio_features("", "4bJQX5w7W4wEnHLmWqUIVY")

songs_i_dislike <- songs_i_dislike %>%
  mutate(mode = ifelse(mode == 0, "Minor", "Major"))

```

```{r chords and keys setup, include=FALSE}
circshift <- function(v, n) {
  if (n == 0) v else c(tail(v, n), head(v, -n))
}

#      C     C#    D     Eb    E     F     F#    G     Ab    A     Bb    B
major_chord <-
  c(   1,    0,    0,    0,    1,    0,    0,    1,    0,    0,    0,    0)
minor_chord <-
  c(   1,    0,    0,    1,    0,    0,    0,    1,    0,    0,    0,    0)
seventh_chord <-
  c(   1,    0,    0,    0,    1,    0,    0,    1,    0,    0,    1,    0)

major_key <-
  c(6.35, 2.23, 3.48, 2.33, 4.38, 4.09, 2.52, 5.19, 2.39, 3.66, 2.29, 2.88)
minor_key <-
  c(6.33, 2.68, 3.52, 5.38, 2.60, 3.53, 2.54, 4.75, 3.98, 2.69, 3.34, 3.17)

chord_templates <-
  tribble(
    ~name, ~template,
    "Gb:7", circshift(seventh_chord, 6),
    "Gb:maj", circshift(major_chord, 6),
    "Bb:min", circshift(minor_chord, 10),
    "Db:maj", circshift(major_chord, 1),
    "F:min", circshift(minor_chord, 5),
    "Ab:7", circshift(seventh_chord, 8),
    "Ab:maj", circshift(major_chord, 8),
    "C:min", circshift(minor_chord, 0),
    "Eb:7", circshift(seventh_chord, 3),
    "Eb:maj", circshift(major_chord, 3),
    "G:min", circshift(minor_chord, 7),
    "Bb:7", circshift(seventh_chord, 10),
    "Bb:maj", circshift(major_chord, 10),
    "D:min", circshift(minor_chord, 2),
    "F:7", circshift(seventh_chord, 5),
    "F:maj", circshift(major_chord, 5),
    "A:min", circshift(minor_chord, 9),
    "C:7", circshift(seventh_chord, 0),
    "C:maj", circshift(major_chord, 0),
    "E:min", circshift(minor_chord, 4),
    "G:7", circshift(seventh_chord, 7),
    "G:maj", circshift(major_chord, 7),
    "B:min", circshift(minor_chord, 11),
    "D:7", circshift(seventh_chord, 2),
    "D:maj", circshift(major_chord, 2),
    "F#:min", circshift(minor_chord, 6),
    "A:7", circshift(seventh_chord, 9),
    "A:maj", circshift(major_chord, 9),
    "C#:min", circshift(minor_chord, 1),
    "E:7", circshift(seventh_chord, 4),
    "E:maj", circshift(major_chord, 4),
    "G#:min", circshift(minor_chord, 8),
    "B:7", circshift(seventh_chord, 11),
    "B:maj", circshift(major_chord, 11),
    "D#:min", circshift(minor_chord, 3)
  )

key_templates <-
  tribble(
    ~name, ~template,
    "Gb:maj", circshift(major_key, 6),
    "Bb:min", circshift(minor_key, 10),
    "Db:maj", circshift(major_key, 1),
    "F:min", circshift(minor_key, 5),
    "Ab:maj", circshift(major_key, 8),
    "C:min", circshift(minor_key, 0),
    "Eb:maj", circshift(major_key, 3),
    "G:min", circshift(minor_key, 7),
    "Bb:maj", circshift(major_key, 10),
    "D:min", circshift(minor_key, 2),
    "F:maj", circshift(major_key, 5),
    "A:min", circshift(minor_key, 9),
    "C:maj", circshift(major_key, 0),
    "E:min", circshift(minor_key, 4),
    "G:maj", circshift(major_key, 7),
    "B:min", circshift(minor_key, 11),
    "D:maj", circshift(major_key, 2),
    "F#:min", circshift(minor_key, 6),
    "A:maj", circshift(major_key, 9),
    "C#:min", circshift(minor_key, 1),
    "E:maj", circshift(major_key, 4),
    "G#:min", circshift(minor_key, 8),
    "B:maj", circshift(major_key, 11),
    "D#:min", circshift(minor_key, 3)
  )

```

Introduction {data-icon="fa-spotify"}
=====================================

Column {data-width=500}
-----------------------------------------------------------------------

### Introduction

Music is an essential part of many people's lives, and with digital music streaming platforms like Spotify, discovering new music has become more accessible than ever. Spotify's algorithm suggests playlists based on users' listening habits and preferences. One of the most popular playlists on Spotify is "New Music Friday," updated every week with new releases from various artists. It is curated by Spotify's editorial team, who select the latest and most popular releases from various genres. However, the question remains: how accurately does this playlist align with personal musical taste and style?

This final portfolio explores the inner workings of Spotify's music recommendation algorithm, attempting to create a predictive model that accurately predicts a user's preferred musical genres and styles. The analysis will focus on two playlists - "Songs I Like" and "Songs I Dislike" - to identify patterns in personal musical taste. The "Songs I Like" playlist is a collection of songs that hold personal meaning, representing various genres such as pop, 70s, 80s, 90s musical, and rock. In contrast, the "Songs I Dislike" playlist consists of heavy metal and drill rap songs that do not match personal taste.

By analyzing the features of songs in both playlists, this portfolio aims to provide insights into personal musical preferences and use this information to train a machine learning model. The goal is to create a predictive model that accurately predicts preferred musical genres and styles. Additionally, the analysis will investigate whether this model can predict which songs from popular playlists like "New Music Friday" would resonate with the user, allowing for a more personalized and tailored listening experience.

In summary, this portfolio tries to offer an understanding of Spotify's music recommendation algorithm and the potential for using machine learning to predict a user's preferred musical genres and styles.

Column {data-width=250}
-----------------------------------------------------------------------

### Corpus: Songs I like

iframe src="https://open.spotify.com/embed/playlist/6pl0C7qbIl5uoY3Tdf82oa?utm_source=generator" width="100%" height="100%" frameBorder="0" allowfullscreen="" allow="autoplay; clipboard-write; encrypted-media; fullscreen; picture-in-picture"></iframe>


Column {data-width=250}
-----------------------------------------------------------------------

### Corpus: Songs I dislike

iframe src="https://open.spotify.com/embed/playlist/4bJQX5w7W4wEnHLmWqUIVY?utm_source=generator&theme=0" width="100%" height="100%" frameBorder="0" allowfullscreen="" allow="autoplay; clipboard-write; encrypted-media; fullscreen; picture-in-picture"></iframe>

Feature analysis {.storyboard data-icon="fa-magnifying-glass"}
=====================================

### Relationship between Valence, Energy, Loudness, and Mode for the songs I like 

```{r}
# Create a scatterplot
songs_i_like %>%
  ggplot(aes(x = valence, y = energy, size = loudness, color = mode)) +
  geom_point(alpha = 0.5) +
  scale_x_continuous(limits = c(0, 1), breaks = c(0, 0.50, 1), minor_breaks = NULL) +
  scale_y_continuous(limits = c(0, 1), breaks = c(0, 0.50, 1), minor_breaks = NULL) +
  scale_color_manual(values = c("darkblue", "lightblue"), name = "Mode", labels = c("Minor", "Major")) +
  scale_size_continuous(trans = "exp", range = c(1, 10), guide = guide_legend(override.aes = list(size = c(1, 3, 5)))) +
  theme_light() +
  labs(x = "Valence", y = "Energy",
       title = "Songs I like",
       subtitle = "Plotting valence, energy, and loudness",
       caption = "Data source: Spotify API\nAuthor: Roan van Blanken")

```

---

Based from this scatterplot it seems like I tend to like songs that are loud, with a range of loudness values represented in the plot. I also seem to be drawn to songs that are high-energy, as they're clustered towards the upper-right portion of the plot where both the energy and loudness values are high.

As for the valence of the songs I like, it appears to be centered around 0.5, but mostly falls between 0.5 and 1.0. This suggests to me that I tend to prefer songs with a positive emotional valence, which could contribute to their appeal.

One interesting thing I noticed in the plot is that the songs I like are equally distributed between major and minor keys, as indicated by the color coding in the plot. This suggests to me that the mode of the songs I like doesn't strongly influence my preference for them.

Overall, this plot provides insight into the features that make songs likable to me, highlighting the importance of loudness, energy, valence, and mode in my musical taste.

### Relationship between Valence, Energy, Loudness, and Mode for the songs I dislike

```{r}
# Create a scatterplot
songs_i_dislike %>%
  ggplot(aes(x = valence, y = energy, size = loudness, color = mode)) +
  geom_point(alpha = 0.5) +
  scale_x_continuous(limits = c(0, 1), breaks = c(0, 0.50, 1), minor_breaks = NULL) +
  scale_y_continuous(limits = c(0, 1), breaks = c(0, 0.50, 1), minor_breaks = NULL) +
  scale_color_manual(values = c("darkred", "lightcoral")) +
  scale_size_continuous(trans = "exp", range = c(1, 10), guide = guide_legend(override.aes = list(size = c(1, 3, 5, 7)))) +
  theme_light() +
  labs(x = "Valence", y = "Energy",
       title = "Songs I dislike",
       subtitle = "Plotting valence, energy, loudness, and mode",
       caption = "Data source: Spotify API\nAuthor: Roan van Blanken")
```

---

Based on the scatterplot of songs I dislike, it seems that they are generally less loud compared to the songs I like. While they still tend to have high energy, they often have a lower valence. This suggests to me that I may prefer songs that are more uplifting or positive in emotional tone, rather than those that are more somber or negative.

Interestingly, like the songs I like, the songs I dislike are similarly distributed between major and minor keys, as indicated by the color coding in the plot. This seems to suggest that the mode of a song is not a major factor in whether or not I like it.

Overall, this scatterplot provides insight into the musical features that I find unappealing in songs, highlighting the importance of loudness, energy, and valence in my musical taste.

### Zooming in on Valence

```{r plot}
plot_ly() %>%
  add_trace(data = songs_i_like, x = ~valence, type = "histogram", 
            histnorm = "probabilty density", name = "Songs I like", 
            marker = list(color = "lightblue", opacity = 0.5)) %>%
  add_trace(data = songs_i_dislike, x = ~valence, type = "histogram", 
            histnorm = "probabilty density", name = "Songs I dislike", 
            marker = list(color = "lightcoral", opacity = 0.5)) %>%
  layout(xaxis = list(title = "Valence"), yaxis = list(title = "Density"),
         title = "Valence Distribution",
         showlegend = TRUE,
         legend = list(title = "", orientation = "h"),
         annotations = list(x = 1, y = -0.15, 
                            text = "Data source: Spotify API\nAuthor: Roan van Blanken",
                            showarrow = FALSE, xref = "paper", yref = "paper",
                            font = list(size = 10),
                            align = "right",
                            xanchor = "right",
                            yanchor = "bottom",
                            pad = list(t = 0, r = 0, b = 10)))

```

---

Songs I like seems to have **more valance** than songs I dislike.

### Zooming in on Energy

```{r}
plot_ly() %>%
  add_trace(data = songs_i_like, x = ~energy, type = "histogram", 
            histnorm = "probabilty density", name = "Songs I like", 
            marker = list(color = "lightgreen", opacity = 0.5)) %>%
  add_trace(data = songs_i_dislike, x = ~energy, type = "histogram", 
            histnorm = "probabilty density", name = "Songs I dislike", 
            marker = list(color = "coral", opacity = 0.5)) %>%
  layout(xaxis = list(title = "Energy"), yaxis = list(title = "Density"),
         title = "Energy Distribution",
         showlegend = TRUE,
         legend = list(title = "", orientation = "h"),
         annotations = list(x = 1, y = -0.15, 
                            text = "Data source: Spotify API\nAuthor: Roan van Blanken",
                            showarrow = FALSE, xref = "paper", yref = "paper",
                            font = list(size = 10),
                            align = "right",
                            xanchor = "right",
                            yanchor = "bottom",
                            pad = list(t = 0, r = 0, b = 10)))
```

---

Songs I like seem to have an energy between **0.7 and 0.9** while the songs I dislike seems to have an energy around **0.95**.

### Zooming in on Loudness

```{r}
plot_ly() %>%
  add_trace(data = songs_i_like, x = ~loudness, type = "histogram", 
            histnorm = "probabilty density", name = "Songs I like", 
            marker = list(color = "#00BFC4", opacity = 0.5)) %>%
  add_trace(data = songs_i_dislike, x = ~loudness, type = "histogram", 
            histnorm = "probabilty density", name = "Songs I dislike", 
            marker = list(color = "#F8766D", opacity = 0.5))%>%
  layout(xaxis = list(title = "Loudness"), yaxis = list(title = "Density"),
         title = "Loudness Distribution",
         showlegend = TRUE,
         legend = list(title = "", orientation = "h"),
         annotations = list(x = 1, y = -0.15, 
                            text = "Data source: Spotify API\nAuthor: Roan van Blanken",
                            showarrow = FALSE, xref = "paper", yref = "paper",
                            font = list(size = 10),
                            align = "right",
                            xanchor = "right",
                            yanchor = "bottom",
                            pad = list(t = 0, r = 0, b = 10)))

```

---

Songs I like seems to be **louder** than song I dislike.

Track-Level Summary {data-icon="fa-chart-bar"}
=====================================

Overview
--------------------------------------------------

### Outliers

The selection of an appropriate corpus is essential to effectively achieve research objectives. In this study, a broad corpus was chosen that included diverse data related to the research question. However, the large volume of data made it difficult to identify significant patterns or trends that could adequately address the research question.

To overcome this challenge, the decision was made to focus on the outliers in the corpus. Specifically, the analysis focused on the extreme cases that were most divergent from the norm in terms of a specific timbre component. This approach allowed for the isolation and study of the outliers, leading to valuable insights and a better understanding of the factors contributing to their unique timbre characteristics. Ultimately, this approach strengthened the analysis and enhanced the quality of the research findings.

Timbre, the quality of sound that distinguishes different musical instruments, is a crucial aspect of music. The analysis of timbre is often performed using spectral content, which measures the relative strengths of various frequency components that make up the sound. In this study, the focus was on a specific timbre component, which was used to isolate and study the outliers in the corpus.

By examining the c2 tab, it can be observed that there are four outliers, with two in each playlist. Specifically, in the **'Songs I like'** playlist, the song 'What I like About You' exhibits the highest timbre in the c02 vector, whereas 'The Sailor's Warning' has the lowest. In contrast, in the **'Songs I dislike'** playlist, 'Murder' has the highest timbre in the c02 vector, while '19 Tini 5' has the lowest.

Column 2 {.tabset}
--------------------------------------------------

### Timbre components

```{r summary_2}
cdata <- readRDS(file="timbre_data.Rda")

plot1 <- cdata %>%
  ggplot(aes(x = factor(basis), y = value, fill = Playlist)) +
  geom_violin() +
  scale_fill_viridis_d() +
  labs(x = "Spotify Timbre Coefficients", y = "", fill = "Playlist") +
  theme_minimal()

ggplotly(plot1)
```

### c01

```{r c01}
# Create table for c01
cdata <- readRDS(file="timbre_data.Rda")

table_c01 <- cdata %>%
  filter(basis == "c01") %>%
  group_by(Playlist) %>%
  slice_min(value, n = 1) %>%
  bind_rows(cdata %>%
              filter(basis == "c01") %>%
              group_by(Playlist) %>%
              slice_max(value, n = 1)) %>%
  select(Playlist, track.name, artists, value) %>%
  rename("Track" = "track.name", "Artists" = "artists")

knitr::kable(table_c01, caption = "Table of maximum and minimum values of c01 for each playlist")

```

### c02

```{r c02}
# Create table for c02
cdata <- readRDS(file="timbre_data.Rda")

table_c02 <- cdata %>%
  filter(basis == "c02") %>%
  group_by(Playlist) %>%
  slice_min(value, n = 1) %>%
  bind_rows(cdata %>%
              filter(basis == "c02") %>%
              group_by(Playlist) %>%
              slice_max(value, n = 1)) %>%
  select(Playlist, track.name, artists, value) %>%
  rename("Track" = "track.name", "Artists" = "artists")

knitr::kable(table_c02, caption = "Table of maximum and minimum values of c02 for each playlist")
```

### c03

```{r c03}
# Create table for c03
cdata <- readRDS(file="timbre_data.Rda")

table_c03 <- cdata %>%
  filter(basis == "c03") %>%
  group_by(Playlist) %>%
  slice_min(value, n = 1) %>%
  bind_rows(cdata %>%
              filter(basis == "c03") %>%
              group_by(Playlist) %>%
              slice_max(value, n = 1)) %>%
  select(Playlist, track.name, artists, value) %>%
  rename("Track" = "track.name", "Artists" = "artists")

knitr::kable(table_c03, caption = "Table of maximum and minimum values of c03 for each playlist")
```

### c04

```{r c04}
# Create table for c04
cdata <- readRDS(file="timbre_data.Rda")

table_c04 <- cdata %>%
  filter(basis == "c04") %>%
  group_by(Playlist) %>%
  slice_min(value, n = 1) %>%
  bind_rows(cdata %>%
              filter(basis == "c04") %>%
              group_by(Playlist) %>%
              slice_max(value, n = 1)) %>%
  select(Playlist, track.name, artists, value) %>%
  rename("Track" = "track.name", "Artists" = "artists")

knitr::kable(table_c04, caption = "Table of maximum and minimum values of c04 for each playlist")
```


Chromagrams {data-icon="fa-music"}
=====================================

Overview
--------------------------------------------------

### Chart A

A chromagram is a visual representation of the distribution of pitches in a musical recording. Comparing the chromagrams of two songs, "State of Unrest" from the playlist of disliked songs and "Shut up and Dance" from the playlist of liked songs, reveals interesting differences in their pitch distribution. In "State of Unrest," there is a strong concentration of pitches around the D chord, indicating a relatively stable harmonic structure. On the other hand, "Shut up and Dance" shows more variation in pitch distribution, with a wider range of pitches around the F#, G#, and A# chords. This suggests that "Shut up and Dance" has a more complex harmonic structure with more varied chord progressions. These differences in pitch distribution could contribute to the overall appeal of the songs and could be further explored in future analyses.

Column 2
--------------------------------------------------

### State of Unrest

```{r}
metal <-
  get_tidy_audio_analysis("3u4djE2yAEkKMWJEUOOJyT") |>
  select(segments) |>
  unnest(segments) |>
  select(start, duration, pitches)

chromogram1 <- metal |>
  mutate(pitches = map(pitches, compmus_normalise, "euclidean")) |>
  compmus_gather_chroma() |> 
  ggplot(
    aes(
      x = start + duration / 2,
      width = duration,
      y = pitch_class,
      fill = value
    )
  ) +
  geom_tile() +
  labs(x = "Time (s)", y = NULL, fill = "Magnitude") +
  theme_minimal() +
  scale_fill_viridis_c()

ggplotly(chromogram1, tooltip = "none", source = "none") %>%
  layout(
    annotations = list(
      list(
        x = 1, y = -0.18,
        text = "Data source: Spotify API\nAuthor: Roan van Blanken",
        showarrow = FALSE,
        xref = "paper",
        yref = "paper",
        font = list(size = 10),
        align = "right",
        xanchor = "right",
        yanchor = "bottom",
        pad = list(t = 0, r = 0, b = 10)
      )
    )
  )
```

### Shut Up and Dance

```{r chromagram 2}
dancewm <-
  get_tidy_audio_analysis("0kzw2tRyuL9rzipi5ntlIy") |>
  select(segments) |>
  unnest(segments) |>
  select(start, duration, pitches)

chromogram2 <- dancewm |>
  mutate(pitches = map(pitches, compmus_normalise, "euclidean")) |>
  compmus_gather_chroma() |> 
  ggplot(
    aes(
      x = start + duration / 2,
      width = duration,
      y = pitch_class,
      fill = value
    )
  ) +
  geom_tile() +
  labs(x = "Time (s)", y = NULL, fill = "Magnitude") +
  theme_minimal() +
  scale_fill_viridis_c()

ggplotly(chromogram2, tooltip = "none", source = "none") %>%
  layout(
    annotations = list(
      list(
        x = 1, y = -0.18,
        text = "Data source: Spotify API\nAuthor: Roan van Blanken",
        showarrow = FALSE,
        xref = "paper",
        yref = "paper",
        font = list(size = 10),
        align = "right",
        xanchor = "right",
        yanchor = "bottom",
        pad = list(t = 0, r = 0, b = 10)
      )
    )
  )
```

Ceptrograms {data-icon="fa-headphones-alt"}
=====================================

Overview
--------------------------------------------------

### Chart A

A ceptrogram is a visual representation of the distribution of timbral characteristics in a musical recording. Comparing the ceptrograms of two songs, "State of Unrest" from the playlist of disliked songs and "Shut up and Dance" from the playlist of liked songs, reveals interesting differences in their timbral distribution. In "State of Unrest," there is a strong concentration of timbral characteristics around a certain range, indicating a relatively stable sonic texture. On the other hand, "Shut up and Dance" shows more variation in timbral distribution, with a wider range of timbral characteristics. This suggests that "Shut up and Dance" has a more complex and varied sound texture. These differences in timbral distribution could contribute to the overall appeal of the songs and could be further explored in future analyses.

Column 2
--------------------------------------------------

### State of Unrest

```{r ceptrogram 1}
metal <-
  get_tidy_audio_analysis("3u4djE2yAEkKMWJEUOOJyT") |> # Change URI.
  compmus_align(bars, segments) |>                     # Change `bars`
  select(bars) |>                                      #   in all three
  unnest(bars) |>                                      #   of these lines.
  mutate(
    pitches =
      map(segments,
        compmus_summarise, pitches,
        method = "rms", norm = "euclidean"              # Change summary & norm.
      )
  ) |>
  mutate(
    timbre =
      map(segments,
        compmus_summarise, timbre,
        method = "rms", norm = "euclidean"              # Change summary & norm.
      )
  )

ceptrogram1 <- metal |>
  compmus_gather_timbre() |>
  ggplot(
    aes(
      x = start + duration / 2,
      width = duration,
      y = basis,
      fill = value
    )
  ) +
  geom_tile() +
  labs(x = "Time (s)", y = NULL, fill = "Magnitude") +
  scale_fill_viridis_c() +                              
  theme_classic()

ggplotly(ceptrogram1, tooltip = "none", source = "none") %>%
  layout(
    annotations = list(
      list(
        x = 1, y = -0.17,
        text = "Data source: Spotify API\nAuthor: Roan van Blanken",
        showarrow = FALSE,
        xref = "paper",
        yref = "paper",
        font = list(size = 10),
        align = "right",
        xanchor = "right",
        yanchor = "bottom",
        pad = list(t = 0, r = 0, b = 10)
      )
    )
  )
```




### Shut Up and Dance

```{r ceptrogram 2}
dancewm <-
  get_tidy_audio_analysis("0kzw2tRyuL9rzipi5ntlIy") |> # Change URI.
  compmus_align(bars, segments) |>                     # Change `bars`
  select(bars) |>                                      #   in all three
  unnest(bars) |>                                      #   of these lines.
  mutate(
    pitches =
      map(segments,
        compmus_summarise, pitches,
        method = "rms", norm = "euclidean"              # Change summary & norm.
      )
  ) |>
  mutate(
    timbre =
      map(segments,
        compmus_summarise, timbre,
        method = "rms", norm = "euclidean"              # Change summary & norm.
      )
  )

ceptrogram2 <- dancewm |>
  compmus_gather_timbre() |>
  ggplot(
    aes(
      x = start + duration / 2,
      width = duration,
      y = basis,
      fill = value
    )
  ) +
  geom_tile() +
  labs(x = "Time (s)", y = NULL, fill = "Magnitude") +
  scale_fill_viridis_c() +                              
  theme_classic()

ggplotly(ceptrogram2, tooltip = "none", source = "none") %>%
  layout(
    annotations = list(
      list(
        x = 1, y = -0.17,
        text = "Data source: Spotify API\nAuthor: Roan van Blanken",
        showarrow = FALSE,
        xref = "paper",
        yref = "paper",
        font = list(size = 10),
        align = "right",
        xanchor = "right",
        yanchor = "bottom",
        pad = list(t = 0, r = 0, b = 10)
      )
    )
  )
```

Self-similarity Matrices
=====================================

Overview {data-width=500}
--------------------------------------------------

### Overview

A self-similarity matrix is a visual representation of the similarity between different sections of a musical recording. Comparing the self-similarity matrices of four songs, "State of Unrest" in both timbre and chroma and "Shut up and Dance" in both timbre and chroma, reveals interesting differences in their internal structure.

In the self-similarity matrix for the timbral characteristics of "State of Unrest," there are clearly defined blocks, indicating repeated patterns in the sound. In contrast, the timbral self-similarity matrix for "Shut up and Dance" shows a more continuous, fluid structure, suggesting a more unpredictable and dynamic sound.

Similarly, the chroma self-similarity matrix for "State of Unrest" shows a highly repetitive structure with clear diagonal lines, indicating the presence of repeated chord progressions. On the other hand, the chroma self-similarity matrix for "Shut up and Dance" shows a more dispersed and varied pattern, indicating a more diverse harmonic structure.

These differences in internal structure could contribute to the overall appeal of the songs and provide insights into the musical composition and arrangement. Further analyses could explore the relationship between these structural characteristics and the emotional and perceptual responses to the music.

column2 {data-width=250}
--------------------------------------------------

### State of Unrest - Self-similarity Matrix (Chroma)

```{r compmus_self_similarity1}
metal |>
  compmus_self_similarity(pitches, "cosine") |> 
  ggplot(
    aes(
      x = xstart + xduration / 2,
      width = xduration,
      y = ystart + yduration / 2,
      height = yduration,
      fill = d
    )
  ) +
  geom_tile() +
  coord_fixed() +
  scale_fill_viridis_c(guide = "none") +
  theme_classic() +
  labs(x = "", y = "")
```


### State of Unrest - Self-similarity Matrix (Timbre)

```{r compmus_self_similarity2}
metal |>
  compmus_self_similarity(timbre, "cosine") |> 
  ggplot(
    aes(
      x = xstart + xduration / 2,
      width = xduration,
      y = ystart + yduration / 2,
      height = yduration,
      fill = d
    )
  ) +
  geom_tile() +
  coord_fixed() +
  scale_fill_viridis_c(guide = "none") +
  theme_classic() +
  labs(x = "", y = "")
```

column3 {data-width=250}
--------------------------------------------------

### Shut Up and Dance - Self-similarity Matrix (Chroma)

```{r compmus_self_similarity3}
dancewm |>
  compmus_self_similarity(pitches, "cosine") |> 
  ggplot(
    aes(
      x = xstart + xduration / 2,
      width = xduration,
      y = ystart + yduration / 2,
      height = yduration,
      fill = d
    )
  ) +
  geom_tile() +
  coord_fixed() +
  scale_fill_viridis_c(guide = "none") +
  theme_classic() +
  labs(x = "", y = "")
```

### Shut Up and Dance - Self-similarity Matrix (Timbre)

```{r compmus_self_similarity4}
dancewm |>
  compmus_self_similarity(timbre, "cosine") |> 
  ggplot(
    aes(
      x = xstart + xduration / 2,
      width = xduration,
      y = ystart + yduration / 2,
      height = yduration,
      fill = d
    )
  ) +
  geom_tile() +
  coord_fixed() +
  scale_fill_viridis_c(guide = "none") +
  theme_classic() +
  labs(x = "", y = "")
```

Chordograms
=====================================

Overview
--------------------------------------------------

### Chart A

WIP

Column 2
--------------------------------------------------

### 

```{r chordogram_1}

```

### 

```{r chordogram_2}

```

Tempograms {data-icon="fas fa-drum"}
=====================================

Overview
--------------------------------------------------

### Chart A

For the Tempograms, I selected the two songs from the 'Songs I like' playlist with the highest and lowest c01 components based on the track-level-summary (see [track-level-summary](index.html#track-level-summary)). Specifically, I chose 'Africa' by Toto with the highest c01 value, and 'Starstruck' by Years & Years with the lowest c01 value. This selection was made to investigate potential differences in tempo between the songs.

From the plots, it can be observed that both songs have a relatively steady tempo with occasional variations. However, 'Starstruck' exhibits more frequent tempo changes, which can be explained by its lower c01 timbre component. The c01 component measures the overall loudness of the song.

In conclusion, the selection of songs based on their c01 component values allowed for an exploration of potential differences in tempo patterns and provided insights into the relationship between c01 values and overall loudness. Additionally, it is worth noting that I tend to prefer songs with a consistent tempo, which may explain my personal preference for songs with similar overall tempos.

Column 2
--------------------------------------------------

### 

```{r tempogram_1}
# Define a custom color palette
blues_purples <- colorRampPalette(c("#2c7bb6", "#abdda4", "#ffffbf", "#fdae61", "#d7191c"))(100)


starstruck_cyclic <- readRDS(file="starstruck_cyclic (Tempogram).Rda")

tempogram_1 <- starstruck_cyclic |>
  ggplot(aes(x = time, y = bpm, fill = power)) +
  geom_raster() +
  ggtitle("Cyclic Tempogram - Years & Years: 'Starstruck'") +
  scale_fill_gradientn(colors = blues_purples) +
  labs(x = "Time (s)", y = "Tempo (BPM)") +
  theme_minimal() +
  scale_color_gradientn(colors = blues_purples)

ggplotly(tempogram_1, tooltip = "none", source = "none")
```

### 

```{r tempogram_2}
# Define a custom color palette
blues_purples <- colorRampPalette(c("#2c7bb6", "#abdda4", "#ffffbf", "#fdae61", "#d7191c"))(100)

africa_cyclic <- readRDS(file="africa_cyclic (Tempogram).Rda")

tempogram_2 <- africa_cyclic |>
  ggplot(aes(x = time, y = bpm, fill = power)) +
  geom_raster() +
  ggtitle("Cyclic Tempogram - Africa: 'Toto'") +
  scale_fill_gradientn(colors = blues_purples) +
  labs(x = "Time (s)", y = "Tempo (BPM)") +
  theme_minimal() +
  scale_color_gradientn(colors = blues_purples)

ggplotly(tempogram_2, tooltip = "none", source = "none")
```

Trained Model {data-icon="ion-options-outline}
=====================================

Overview
--------------------------------------------------

### Chart A

WIP

Column 2
--------------------------------------------------

### 

```{r trained_model_1}

```

### 

```{r trained_model_2}

```

Conclusion {data-icon="ion-document"}
=====================================

Column 1
--------------------------------------------------

### Conclusion

Based on the insights gained from the feature analysis, it appears that there are clear patterns in the musical features that I find appealing and unappealing in songs. This raises the possibility of developing a classification model to predict whether or not I would like a particular song based on its acoustic features.

For example, a decision tree model could be trained on a dataset of songs that I have rated as either liked or disliked, using features such as loudness, energy, valence, and mode as predictors. The resulting model could then be used to predict the likelihood of me liking a new song based on its acoustic features.

While the plots provide some valuable insights into the musical features that I find appealing or unappealing, it's important to note that these are just a few of the many features that could potentially influence my musical taste. A more accurate classification model would need to take into account a broader range of features, such as tempo, rhythm, instrumentation, and genre, among others. Additionally, the model would need to be trained on a larger and more diverse set of songs to ensure that it can accurately classify songs that I like or dislike across a wider range of styles and genres. Nevertheless, the insights gained from these plots provide a good starting point for developing a more comprehensive model of my musical taste.