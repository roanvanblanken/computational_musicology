---
title: "Computational Musicology - Final Portfolio"
author: "Roan van Blanken"
output: 
  flexdashboard::flex_dashboard:
    orientation: columns
    self_contained: false
---

```{r setup, include=FALSE}
library(flexdashboard)
library(spotifyr)
library(purrr)
library(dplyr)
library(ggplot2)
library(plotly)
library(tidyverse)
library(compmus)
library(fontawesome)

# Get playlists
songs_i_like <- get_playlist_audio_features("", "6pl0C7qbIl5uoY3Tdf82oa")

songs_i_like <- songs_i_like %>%
  mutate(mode = ifelse(mode == 0, "Minor", "Major"))

songs_i_dislike <- get_playlist_audio_features("", "4bJQX5w7W4wEnHLmWqUIVY")

songs_i_dislike <- songs_i_dislike %>%
  mutate(mode = ifelse(mode == 0, "Minor", "Major"))

# Define a custom color palette
random_palette <- colorRampPalette(c("#2c7bb6", "#abdda4", "#ffffbf", "#fdae61", "#d7191c"))(100)
blue_red <- colorRampPalette(c("#00008B", "#ADD8E6", "#E6FFFF", "#FFE4E1", "#FF6347", "#8B0000"))(100)
```

```{r chords and keys setup, include=FALSE}
circshift <- function(v, n) {
  if (n == 0) v else c(tail(v, n), head(v, -n))
}

#      C     C#    D     Eb    E     F     F#    G     Ab    A     Bb    B
major_chord <-
  c(   1,    0,    0,    0,    1,    0,    0,    1,    0,    0,    0,    0)
minor_chord <-
  c(   1,    0,    0,    1,    0,    0,    0,    1,    0,    0,    0,    0)
seventh_chord <-
  c(   1,    0,    0,    0,    1,    0,    0,    1,    0,    0,    1,    0)

major_key <-
  c(6.35, 2.23, 3.48, 2.33, 4.38, 4.09, 2.52, 5.19, 2.39, 3.66, 2.29, 2.88)
minor_key <-
  c(6.33, 2.68, 3.52, 5.38, 2.60, 3.53, 2.54, 4.75, 3.98, 2.69, 3.34, 3.17)

chord_templates <-
  tribble(
    ~name, ~template,
    "Gb:7", circshift(seventh_chord, 6),
    "Gb:maj", circshift(major_chord, 6),
    "Bb:min", circshift(minor_chord, 10),
    "Db:maj", circshift(major_chord, 1),
    "F:min", circshift(minor_chord, 5),
    "Ab:7", circshift(seventh_chord, 8),
    "Ab:maj", circshift(major_chord, 8),
    "C:min", circshift(minor_chord, 0),
    "Eb:7", circshift(seventh_chord, 3),
    "Eb:maj", circshift(major_chord, 3),
    "G:min", circshift(minor_chord, 7),
    "Bb:7", circshift(seventh_chord, 10),
    "Bb:maj", circshift(major_chord, 10),
    "D:min", circshift(minor_chord, 2),
    "F:7", circshift(seventh_chord, 5),
    "F:maj", circshift(major_chord, 5),
    "A:min", circshift(minor_chord, 9),
    "C:7", circshift(seventh_chord, 0),
    "C:maj", circshift(major_chord, 0),
    "E:min", circshift(minor_chord, 4),
    "G:7", circshift(seventh_chord, 7),
    "G:maj", circshift(major_chord, 7),
    "B:min", circshift(minor_chord, 11),
    "D:7", circshift(seventh_chord, 2),
    "D:maj", circshift(major_chord, 2),
    "F#:min", circshift(minor_chord, 6),
    "A:7", circshift(seventh_chord, 9),
    "A:maj", circshift(major_chord, 9),
    "C#:min", circshift(minor_chord, 1),
    "E:7", circshift(seventh_chord, 4),
    "E:maj", circshift(major_chord, 4),
    "G#:min", circshift(minor_chord, 8),
    "B:7", circshift(seventh_chord, 11),
    "B:maj", circshift(major_chord, 11),
    "D#:min", circshift(minor_chord, 3)
  )

key_templates <-
  tribble(
    ~name, ~template,
    "Gb:maj", circshift(major_key, 6),
    "Bb:min", circshift(minor_key, 10),
    "Db:maj", circshift(major_key, 1),
    "F:min", circshift(minor_key, 5),
    "Ab:maj", circshift(major_key, 8),
    "C:min", circshift(minor_key, 0),
    "Eb:maj", circshift(major_key, 3),
    "G:min", circshift(minor_key, 7),
    "Bb:maj", circshift(major_key, 10),
    "D:min", circshift(minor_key, 2),
    "F:maj", circshift(major_key, 5),
    "A:min", circshift(minor_key, 9),
    "C:maj", circshift(major_key, 0),
    "E:min", circshift(minor_key, 4),
    "G:maj", circshift(major_key, 7),
    "B:min", circshift(minor_key, 11),
    "D:maj", circshift(major_key, 2),
    "F#:min", circshift(minor_key, 6),
    "A:maj", circshift(major_key, 9),
    "C#:min", circshift(minor_key, 1),
    "E:maj", circshift(major_key, 4),
    "G#:min", circshift(minor_key, 8),
    "B:maj", circshift(major_key, 11),
    "D#:min", circshift(minor_key, 3)
  )

```

Introduction {data-icon="fa-spotify"}
=====================================

Column {data-width=500}
-----------------------------------------------------------------------

### Introduction

Music is an essential part of many people's lives, and with digital music streaming platforms like Spotify, discovering new music has become more accessible than ever. Spotify's algorithm suggests playlists based on users' listening habits and preferences. One of the most popular playlists on Spotify is "New Music Friday," updated every week with new releases from various artists. It is curated by Spotify's editorial team, who select the latest and most popular releases from various genres. However, the question remains: how accurately does this playlist align with personal musical taste and style?

This final portfolio explores the inner workings of Spotify's music recommendation algorithm, attempting to create a predictive model that accurately predicts a user's preferred musical genres and styles. The analysis will focus on two playlists - "Songs I Like" and "Songs I Dislike" - to identify patterns in personal musical taste. The "Songs I Like" playlist is a collection of songs that hold personal meaning, representing various genres such as pop, 70s, 80s, 90s musical, and rock. In contrast, the "Songs I Dislike" playlist consists of heavy metal and drill rap songs that do not match personal taste.

By analyzing the features of songs in both playlists, this portfolio aims to provide insights into personal musical preferences and use this information to train a machine learning model. The goal is to create a predictive model that accurately predicts preferred musical genres and styles. Additionally, the analysis will investigate whether this model can predict which songs from popular playlists like "New Music Friday" would resonate with the user, allowing for a more personalized and tailored listening experience.

In summary, this portfolio tries to offer an understanding of Spotify's music recommendation algorithm and the potential for using machine learning to predict a user's preferred musical genres and styles.

Column {data-width=250}
-----------------------------------------------------------------------

### Corpus: Songs I like

iframe src="https://open.spotify.com/embed/playlist/6pl0C7qbIl5uoY3Tdf82oa?utm_source=generator" width="100%" height="100%" frameBorder="0" allowfullscreen="" allow="autoplay; clipboard-write; encrypted-media; fullscreen; picture-in-picture"></iframe>


Column {data-width=250}
-----------------------------------------------------------------------

### Corpus: Songs I dislike

iframe src="https://open.spotify.com/embed/playlist/4bJQX5w7W4wEnHLmWqUIVY?utm_source=generator&theme=0" width="100%" height="100%" frameBorder="0" allowfullscreen="" allow="autoplay; clipboard-write; encrypted-media; fullscreen; picture-in-picture"></iframe>

Feature analysis {.storyboard data-icon="fa-magnifying-glass"}
=====================================

### Relationship between Valence, Energy, Loudness, and Mode for the songs I like 

```{r songs_i_like plot}
# Create a scatterplot
songs_i_like %>%
  ggplot(aes(x = valence, y = energy, size = loudness, color = mode)) +
  geom_point(alpha = 0.5) +
  scale_x_continuous(limits = c(0, 1), breaks = c(0, 0.50, 1), minor_breaks = NULL) +
  scale_y_continuous(limits = c(0, 1), breaks = c(0, 0.50, 1), minor_breaks = NULL) +
  scale_color_manual(values = c("darkblue", "lightblue"), name = "Mode", labels = c("Minor", "Major")) +
  scale_size_continuous(trans = "exp", range = c(1, 10), guide = guide_legend(override.aes = list(size = c(1, 3, 5)))) +
  theme_light() +
  labs(x = "Valence", y = "Energy",
       title = "Songs I like",
       subtitle = "Plotting valence, energy, and loudness",
       caption = "Data source: Spotify API\nAuthor: Roan van Blanken")

```

---

Spotify's track-level features, such as valence, danceability, energy, and loudness, offer valuable insights into the underlying patterns of my personal music preferences. I selected these features as they are commonly used to describe the overall mood, intensity, and emotional tone of a song, allowing us to better understand the characteristics that define the songs I like and dislike [(Serra et al.)](https://pubmed.ncbi.nlm.nih.gov/22837813/). By examining these features, we can gain a deeper understanding of the traits that influence my musical taste.

Based on the first scatterplot, it seems like I tend to like songs that are loud, with a range of loudness values represented in the plot. I also seem to be drawn to songs that are high-energy, as they're clustered towards the upper-right portion of the plot where both the energy and loudness values are high.

As for the valence of the songs I like, it appears to be centered around 0.5, but mostly falls between 0.5 and 1.0. This suggests that I tend to prefer songs with a positive emotional valence, which could contribute to their appeal.

One interesting thing I noticed in the plot is that the songs I like are equally distributed between major and minor keys, as indicated by the color coding in the plot. This suggests that the mode of the songs I like doesn't strongly influence my preference for them.

### Relationship between Valence, Energy, Loudness, and Mode for the songs I dislike

```{r songs_i_dislike plot}
# Create a scatterplot
songs_i_dislike %>%
  ggplot(aes(x = valence, y = energy, size = loudness, color = mode)) +
  geom_point(alpha = 0.5) +
  scale_x_continuous(limits = c(0, 1), breaks = c(0, 0.50, 1), minor_breaks = NULL) +
  scale_y_continuous(limits = c(0, 1), breaks = c(0, 0.50, 1), minor_breaks = NULL) +
  scale_color_manual(values = c("darkred", "lightcoral")) +
  scale_size_continuous(trans = "exp", range = c(1, 10), guide = guide_legend(override.aes = list(size = c(1, 3, 5, 7)))) +
  theme_light() +
  labs(x = "Valence", y = "Energy",
       title = "Songs I dislike",
       subtitle = "Plotting valence, energy, loudness, and mode",
       caption = "Data source: Spotify API\nAuthor: Roan van Blanken")
```

---

Looking at the scatterplot of songs I don't like, I can see that they're usually not as loud as the songs I enjoy. Even though they still have high energy levels, they often have lower valence. This makes me believe that I might like songs that have a more positive and uplifting vibe, rather than ones that are more downbeat or sad.

It's interesting to see that, just like the songs I like, the songs I don't like are spread out pretty evenly between major and minor keys. This tells me that the key of a song doesn't really play a big role in whether I like it or not.

Another thing I noticed is that the songs I don't like seem to have a smaller range of loudness and energy compared to the songs I do like. This could mean that I'm more open to different levels of intensity and dynamics when it comes to music I enjoy, while the music I don't like might have more in common in terms of how loud and energetic they are.

### Zooming in on Valence

```{r valence_plot}
plot_ly() %>%
  add_trace(data = songs_i_like, x = ~valence, type = "histogram", 
            histnorm = "probabilty density", name = "Songs I like", 
            marker = list(color = "lightblue", opacity = 0.5)) %>%
  add_trace(data = songs_i_dislike, x = ~valence, type = "histogram", 
            histnorm = "probabilty density", name = "Songs I dislike", 
            marker = list(color = "lightcoral", opacity = 0.5)) %>%
  layout(xaxis = list(title = "Valence"), yaxis = list(title = "Density"),
         title = "Valence Distribution",
         showlegend = TRUE,
         legend = list(title = "", orientation = "h"))
```

---

This plot provides a visual representation of how valence is distributed among two different sets of songs: 'Songs I like' and 'Songs I dislike'.

Valence is a measure of how positive or negative a musical piece sounds, with a scale that ranges from negative to positive values. The plot shows two histograms, one in light blue representing the valence distribution for the songs that I like, and the other in light coral representing the valence distribution for the songs that I dislike.

By looking at the plot, we can see that the valence distribution for the songs that I like is skewed towards the positive end of the spectrum, indicating that the songs the I like tend to have a more positive sound. In contrast, the valence distribution for the songs that I dislike is more skewed towards the negative end of the spectrum, suggesting that the songs I dislike have a more negative sound.

### Zooming in on Energy

```{r energy_plot}
plot_ly() %>%
  add_trace(data = songs_i_like, x = ~energy, type = "histogram", 
            histnorm = "probabilty density", name = "Songs I like", 
            marker = list(color = "lightgreen", opacity = 0.5)) %>%
  add_trace(data = songs_i_dislike, x = ~energy, type = "histogram", 
            histnorm = "probabilty density", name = "Songs I dislike", 
            marker = list(color = "coral", opacity = 0.5)) %>%
  layout(xaxis = list(title = "Energy"), yaxis = list(title = "Density"),
         title = "Energy Distribution",
         showlegend = TRUE,
         legend = list(title = "", orientation = "h"))
```

---

This plot provides insight into the energy distribution of two different sets of songs: 'Songs I like' and 'Songs I dislike'. Energy is a measure of how intense and active a musical piece is, with a scale that ranges from low to high values. The plot displays two histograms, one in light green representing the energy distribution for the songs that I like, and the other in coral representing the energy distribution for the songs that I dislike.

Upon examining the plot, we can see that the energy distribution for the songs that I like is concentrated between 0.7 and 0.9, with a peak around 0.8. This indicates that the songs that I enjoy tend to have a moderate level of energy, without being too intense or too mellow. In contrast, the energy distribution for the songs that I dislike is concentrated around 0.95, indicating that the songs that I find unfavorable tend to be more energetic.

The findings of this plot suggest that energy may be an important factor in shaping my musical preferences. However, it is essential to note that energy is just one of many factors that can influence an individual's musical tastes.

### Zooming in on Loudness

```{r loudness_plot}
plot_ly() %>%
  add_trace(data = songs_i_like, x = ~loudness, type = "histogram", 
            histnorm = "probabilty density", name = "Songs I like", 
            marker = list(color = "#00BFC4", opacity = 0.5)) %>%
  add_trace(data = songs_i_dislike, x = ~loudness, type = "histogram", 
            histnorm = "probabilty density", name = "Songs I dislike", 
            marker = list(color = "#F8766D", opacity = 0.5))%>%
  layout(xaxis = list(title = "Loudness"), yaxis = list(title = "Density"),
         title = "Loudness Distribution",
         showlegend = TRUE,
         legend = list(title = "", orientation = "h"))
```

---

Musical preferences are complex and can be influenced by various factors, including the loudness of a musical piece. Loudness is an essential characteristic of music that can affect how it is perceived by the listener. It refers to the volume or intensity of a musical piece and is typically measured in decibels.

The plot that highlights the difference in loudness between songs that I like and songs that I dislike may provide insight into how loudness influences my musical preferences. The plot may indicate that my preferred songs tend to be louder than songs that I dislike, suggesting that loudness is an essential characteristic of my musical taste. However, it is crucial to remember that loudness is just one factor that can influence musical preferences.

Track-Level Summary {data-icon="fa-chart-bar"}
=====================================

Overview
--------------------------------------------------

### Outliers

In any research study, selecting an appropriate corpus is crucial in effectively achieving the research objectives. In the present study, a broad corpus was chosen, which included diverse data related to the research question. However, the large volume of data made it challenging to identify significant patterns or trends that could adequately address the research question.

To overcome this challenge, the decision was made to focus on the outliers in the corpus. Specifically, the analysis focused on the extreme cases that were most divergent from the norm in terms of a specific timbre component. This approach allowed for the isolation and study of the outliers, leading to valuable insights and a better understanding of the factors contributing to their unique timbre characteristics. Ultimately, this approach strengthened the analysis and enhanced the quality of the research findings.

In this study, timbre, the quality of sound that distinguishes different musical instruments, was analyzed using spectral content, which measures the relative strengths of various frequency components that make up the sound. The study focused on a specific timbre component, which was used to isolate and study the outliers in the corpus.

To further investigate this approach, tables were generated to identify the maximum and minimum values of specific timbre components for each playlist. The tables for the maximum and minimum values of c01, c02, c03, and c04 for both the 'Songs I like' and 'Songs I dislike' playlists showed notable differences in timbre components between the two playlists. For example, the table of maximum and minimum values of c02 for each playlist showed that the highest and lowest timbre values for 'Songs I like' were exhibited by the songs ['What I Like About You'](https://open.spotify.com/track/4ebcE2SmkG7nplvzFAWRu7?si=c432fccad51845ae), and ['The Sailor’s Warning'](https://open.spotify.com/track/3bgmoyCHa6v3bMMsgK1rWh?si=d7b40ddaef5344a1), respectively, while the highest and lowest timbre values for 'Songs I dislike' were exhibited by the songs ['Murder'](https://open.spotify.com/track/0wFPKFtBes3NTyTW4bkjrz?si=796b185ece5d4a75) and ['19 Tini 5'](https://open.spotify.com/track/73myy2XXpvC2bHi4Fikksr?si=775791f95d43461a), respectively.

These tables provide insight into how specific timbre components differ between songs that I like and songs that I dislike, which helps to support the approach of focusing on the outliers in the corpus to gain valuable insights into timbre characteristics.

Column 2 {.tabset}
--------------------------------------------------

### Timbre components

```{r timbre_summary}
timbre_data <- readRDS(file="data/timbre_data.Rda")

plot1 <- timbre_data %>%
  ggplot(aes(x = factor(basis), y = value, fill = Playlist)) +
  geom_violin() +
  scale_fill_viridis_d() +
  labs(x = "Spotify Timbre Coefficients", y = "", fill = "Playlist") +
  theme_minimal()

ggplotly(plot1)
```

### c01

```{r c01}
# Create table for c01
table_c01 <- timbre_data %>%
  filter(basis == "c01") %>%
  group_by(Playlist) %>%
  slice_min(value, n = 1) %>%
  bind_rows(timbre_data %>%
              filter(basis == "c01") %>%
              group_by(Playlist) %>%
              slice_max(value, n = 1)) %>%
  select(Playlist, track.name, artists, value) %>%
  rename("Track" = "track.name", "Artists" = "artists")

knitr::kable(table_c01, caption = "Table of maximum and minimum values of c01 for each playlist")

```

### c02

```{r c02}
# Create table for c02
table_c02 <- timbre_data %>%
  filter(basis == "c02") %>%
  group_by(Playlist) %>%
  slice_min(value, n = 1) %>%
  bind_rows(timbre_data %>%
              filter(basis == "c02") %>%
              group_by(Playlist) %>%
              slice_max(value, n = 1)) %>%
  select(Playlist, track.name, artists, value) %>%
  rename("Track" = "track.name", "Artists" = "artists")

knitr::kable(table_c02, caption = "Table of maximum and minimum values of c02 for each playlist")
```

### c03

```{r c03}
# Create table for c03
table_c03 <- timbre_data %>%
  filter(basis == "c03") %>%
  group_by(Playlist) %>%
  slice_min(value, n = 1) %>%
  bind_rows(timbre_data %>%
              filter(basis == "c03") %>%
              group_by(Playlist) %>%
              slice_max(value, n = 1)) %>%
  select(Playlist, track.name, artists, value) %>%
  rename("Track" = "track.name", "Artists" = "artists")

knitr::kable(table_c03, caption = "Table of maximum and minimum values of c03 for each playlist")
```

### c04

```{r c04}
# Create table for c04
table_c04 <- timbre_data %>%
  filter(basis == "c04") %>%
  group_by(Playlist) %>%
  slice_min(value, n = 1) %>%
  bind_rows(timbre_data %>%
              filter(basis == "c04") %>%
              group_by(Playlist) %>%
              slice_max(value, n = 1)) %>%
  select(Playlist, track.name, artists, value) %>%
  rename("Track" = "track.name", "Artists" = "artists")

knitr::kable(table_c04, caption = "Table of maximum and minimum values of c04 for each playlist")
```


Chromagrams {data-icon="fa-music"}
=====================================

Column 1
--------------------------------------------------

###

A chromagram is a visual representation of the distribution of pitches in a musical recording. Comparing the chromagrams of two songs, "State of Unrest" from the playlist of disliked songs and "Shut up and Dance" from the playlist of liked songs, reveals interesting differences in their pitch distribution. In "State of Unrest," there is a strong concentration of pitches around the D chord, indicating a relatively stable harmonic structure. On the other hand, "Shut up and Dance" shows more variation in pitch distribution, with a wider range of pitches around the F#, G#, and A# chords. This suggests that "Shut up and Dance" has a more complex harmonic structure with more varied chord progressions. These differences in pitch distribution could contribute to the overall appeal of the songs and could be further explored in future analyses.

Column 2
--------------------------------------------------

###

```{r chromagram1}
wilay <-
  get_tidy_audio_analysis("4ebcE2SmkG7nplvzFAWRu7") |>
  select(segments) |>
  unnest(segments) |>
  select(start, duration, pitches)

chromogram1 <- wilay |>
  mutate(pitches = map(pitches, compmus_normalise, "euclidean")) |>
  compmus_gather_chroma() |> 
  ggplot(
    aes(
      x = start + duration / 2,
      width = duration,
      y = pitch_class,
      fill = value
    )
  ) +
  geom_tile() +
  ggtitle("Chromagram - The Romantics: 'What I Like About You'") +
  scale_fill_gradientn(colors = random_palette) +
  labs(x = "Time (s)", y = NULL, fill = "Magnitude") +
  theme_minimal() +
  scale_color_gradientn(colors = random_palette)

ggplotly(chromogram1, tooltip = "none", source = "none")
```

###

```{r chromagram 2}
murder <-
  get_tidy_audio_analysis("0wFPKFtBes3NTyTW4bkjrz") |>
  select(segments) |>
  unnest(segments) |>
  select(start, duration, pitches)

chromogram2 <- murder |>
  mutate(pitches = map(pitches, compmus_normalise, "euclidean")) |>
  compmus_gather_chroma() |> 
  ggplot(
    aes(
      x = start + duration / 2,
      width = duration,
      y = pitch_class,
      fill = value
    )
  ) +
  geom_tile() +
  ggtitle("Chromagram - DJ Squeeky, Tom Skeemask, GK: 'Murder'") +
  scale_fill_gradientn(colors = random_palette) +
  labs(x = "Time (s)", y = NULL, fill = "Magnitude") +
  theme_minimal() +
  scale_color_gradientn(colors = random_palette)

ggplotly(chromogram2, tooltip = "none", source = "none")
```

Ceptrograms {data-icon="fa-headphones-alt"}
=====================================

Overview
--------------------------------------------------

### Chart A

A ceptrogram is a visual representation of the distribution of timbral characteristics in a musical recording. Comparing the ceptrograms of two songs, "State of Unrest" from the playlist of disliked songs and "Shut up and Dance" from the playlist of liked songs, reveals interesting differences in their timbral distribution. In "State of Unrest," there is a strong concentration of timbral characteristics around a certain range, indicating a relatively stable sonic texture. On the other hand, "Shut up and Dance" shows more variation in timbral distribution, with a wider range of timbral characteristics. This suggests that "Shut up and Dance" has a more complex and varied sound texture. These differences in timbral distribution could contribute to the overall appeal of the songs and could be further explored in future analyses.

Column 2
--------------------------------------------------

###

```{r ceptrogram 1}
state_of_unrest <- readRDS(file="data/Ceptrogram (state_of_unrest).Rda")

ceptrogram1 <- state_of_unrest |>
  ggplot(aes(x = start + duration / 2, width = duration, y = basis, fill = value)) +
  geom_tile() +
  ggtitle("Ceptrogram - Lamb of God, Kreator: 'State of Unrest'") +
  scale_fill_gradientn(colors = random_palette) +
  labs(x = "Time (s)", y = NULL, fill = "Magnitude") +
  theme_minimal() +
  scale_color_gradientn(colors = random_palette)

ggplotly(ceptrogram1, tooltip = "none", source = "none")
```

###

```{r ceptrogram 2}
shutupanddance <- readRDS(file="data/Ceptrogram (shutupanddance).Rda")

ceptrogram2 <- shutupanddance |>
  ggplot(aes(x = start + duration / 2, width = duration, y = basis, fill = value)) +
  geom_tile() +
  ggtitle("Ceptrogram - WALK THE MOON: 'Shut up and Dance'") +
  scale_fill_gradientn(colors = random_palette) +
  labs(x = "Time (s)", y = NULL, fill = "Magnitude") +
  theme_minimal() +
  scale_color_gradientn(colors = random_palette)

ggplotly(ceptrogram2, tooltip = "none", source = "none")
```

Self-similarity Matrices {data-icon="fas fa-share-from-square"}
=====================================

Overview {data-width=500}
--------------------------------------------------

###

A self-similarity matrix is a visual representation of the similarity between different sections of a musical recording. Comparing the self-similarity matrices of four songs, "State of Unrest" in both timbre and chroma and "Shut up and Dance" in both timbre and chroma, reveals interesting differences in their internal structure.

In the self-similarity matrix for the timbral characteristics of "State of Unrest," there are clearly defined blocks, indicating repeated patterns in the sound. In contrast, the timbral self-similarity matrix for "Shut up and Dance" shows a more continuous, fluid structure, suggesting a more unpredictable and dynamic sound.

Similarly, the chroma self-similarity matrix for "State of Unrest" shows a highly repetitive structure with clear diagonal lines, indicating the presence of repeated chord progressions. On the other hand, the chroma self-similarity matrix for "Shut up and Dance" shows a more dispersed and varied pattern, indicating a more diverse harmonic structure.

These differences in internal structure could contribute to the overall appeal of the songs and provide insights into the musical composition and arrangement. Further analyses could explore the relationship between these structural characteristics and the emotional and perceptual responses to the music.

column2 {data-width=250}
--------------------------------------------------

###

```{r compmus_self_similarity1}
state_of_unrest_chroma <- readRDS("data/Self-similarity Matrix (state_of_unrest_chroma).Rda")

ssm1 <- state_of_unrest_chroma |> 
  ggplot(
    aes(
      x = xstart + xduration / 2,
      width = xduration,
      y = ystart + yduration / 2,
      height = yduration,
      fill = d
    )
  ) +
  geom_tile() +
  ggtitle("Self-similarity Matrix (Chroma)\nLamb of God, Kreator: 'State of Unrest'") +
  scale_fill_gradientn(colors = blue_red) +
  labs(x = NULL, y = NULL, fill = "") +
  theme_minimal() +
  scale_color_gradientn(colors = blue_red) +
  theme(plot.title = element_text(size = 10))

ggplotly(ssm1, tooltip = "none", source = "none")
```


###

```{r compmus_self_similarity2}
state_of_unrest_timbre <- readRDS("data/Self-similarity Matrix (state_of_unrest_timbre).Rda")

ssm2 <- state_of_unrest_timbre |> 
  ggplot(
    aes(
      x = xstart + xduration / 2,
      width = xduration,
      y = ystart + yduration / 2,
      height = yduration,
      fill = d
    )
  ) +
  geom_tile() +
  ggtitle("Self-similarity Matrix (Timbre)\nLamb of God, Kreator: 'State of Unrest'") +
  scale_fill_gradientn(colors = blue_red) +
  labs(x = NULL, y = NULL, fill = "") +
  theme_minimal() +
  scale_color_gradientn(colors = blue_red) +
  theme(plot.title = element_text(size = 10))

ggplotly(ssm2, tooltip = "none", source = "none")
```

column3 {data-width=250}
--------------------------------------------------

###

```{r compmus_self_similarity3}
shutupanddance_chroma <- readRDS("data/Self-similarity Matrix (shutupanddance_chroma).Rda")

ssm3 <- shutupanddance_chroma |> 
  ggplot(
    aes(
      x = xstart + xduration / 2,
      width = xduration,
      y = ystart + yduration / 2,
      height = yduration,
      fill = d
    )
  ) +
  geom_tile() +
  ggtitle("Self-similarity Matrix (Chroma)\nWALK THE MOON: 'Shut up and Dance'") +
  scale_fill_gradientn(colors = blue_red) +
  labs(x = NULL, y = NULL, fill = "") +
  theme_minimal() +
  scale_color_gradientn(colors = blue_red) +
  theme(plot.title = element_text(size = 10))

ggplotly(ssm3, tooltip = "none", source = "none")
```

###

```{r compmus_self_similarity4}
shutupanddance_timbre <- readRDS("data/Self-similarity Matrix (shutupanddance_timbre).Rda")

ssm4 <- shutupanddance_timbre |> 
  ggplot(
    aes(
      x = xstart + xduration / 2,
      width = xduration,
      y = ystart + yduration / 2,
      height = yduration,
      fill = d
    )
  ) +
  geom_tile() +
  ggtitle("Self-similarity Matrix (Timbre)\nWALK THE MOON: 'Shut up and Dance'") +
  scale_fill_gradientn(colors = blue_red) +
  labs(x = NULL, y = NULL, fill = "") +
  theme_minimal() +
  scale_color_gradientn(colors = blue_red) +
  theme(plot.title = element_text(size = 10))

ggplotly(ssm4, tooltip = "none", source = "none")
```

Chordograms
=====================================

Overview
--------------------------------------------------

### Chart A

WIP

Column 2
--------------------------------------------------

### 

```{r chordogram_1}

```

### 

```{r chordogram_2}

```

Tempograms {data-icon="fas fa-drum"}
=====================================

Overview
--------------------------------------------------

### Chart A

For the Tempograms, I selected the two songs from the 'Songs I like' playlist with the highest and lowest c01 components based on the track-level-summary (see [track-level-summary](index.html#track-level-summary)). Specifically, I chose 'Africa' by Toto with the highest c01 value, and 'Starstruck' by Years & Years with the lowest c01 value. This selection was made to investigate potential differences in tempo between the songs.

From the plots, it can be observed that both songs have a relatively steady tempo with occasional variations. However, 'Starstruck' exhibits more frequent tempo changes, which can be explained by its lower c01 timbre component. The c01 component measures the overall loudness of the song.

In conclusion, the selection of songs based on their c01 component values allowed for an exploration of potential differences in tempo patterns and provided insights into the relationship between c01 values and overall loudness. Additionally, it is worth noting that I tend to prefer songs with a consistent tempo, which may explain my personal preference for songs with similar overall tempos.

Column 2
--------------------------------------------------

### 

```{r tempogram_1}
starstruck_cyclic <- readRDS(file="data/Tempogram (starstruck_cyclic).Rda")

tempogram_1 <- starstruck_cyclic |>
  ggplot(aes(x = time, y = bpm, fill = power)) +
  geom_raster() +
  ggtitle("Cyclic Tempogram - Years & Years: 'Starstruck'") +
  scale_fill_gradientn(colors = random_palette) +
  labs(x = "Time (s)", y = "Tempo (BPM)") +
  theme_minimal() +
  scale_color_gradientn(colors = random_palette)

ggplotly(tempogram_1, tooltip = "none", source = "none")
```

### 

```{r tempogram_2}
africa_cyclic <- readRDS(file="data/Tempogram (africa_cyclic).Rda")

tempogram_2 <- africa_cyclic |>
  ggplot(aes(x = time, y = bpm, fill = power)) +
  geom_raster() +
  ggtitle("Cyclic Tempogram - Africa: 'Toto'") +
  scale_fill_gradientn(colors = random_palette) +
  labs(x = "Time (s)", y = "Tempo (BPM)") +
  theme_minimal() +
  scale_color_gradientn(colors = random_palette)

ggplotly(tempogram_2, tooltip = "none", source = "none")
```

Trained Model {data-icon="ion-options-outline}
=====================================

Overview
--------------------------------------------------

### Chart A

WIP

Column 2
--------------------------------------------------

### 

```{r trained_model_1}

```

### 

```{r trained_model_2}

```

Conclusion {data-icon="ion-document"}
=====================================

Column 1
--------------------------------------------------

### Conclusion

Based on the insights gained from the feature analysis, it appears that there are clear patterns in the musical features that I find appealing and unappealing in songs. This raises the possibility of developing a classification model to predict whether or not I would like a particular song based on its acoustic features.

For example, a decision tree model could be trained on a dataset of songs that I have rated as either liked or disliked, using features such as loudness, energy, valence, and mode as predictors. The resulting model could then be used to predict the likelihood of me liking a new song based on its acoustic features.

While the plots provide some valuable insights into the musical features that I find appealing or unappealing, it's important to note that these are just a few of the many features that could potentially influence my musical taste. A more accurate classification model would need to take into account a broader range of features, such as tempo, rhythm, instrumentation, and genre, among others. Additionally, the model would need to be trained on a larger and more diverse set of songs to ensure that it can accurately classify songs that I like or dislike across a wider range of styles and genres. Nevertheless, the insights gained from these plots provide a good starting point for developing a more comprehensive model of my musical taste.