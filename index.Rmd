---
title: "Computational Musicology - Final Portfolio"
author: "Roan van Blanken"
output: 
  flexdashboard::flex_dashboard:
    orientation: columns
    self_contained: false
    
---

```{r setup, include=FALSE}
library(compmus)
library(dplyr)
library(flexdashboard)
library(fontawesome)
library(ggdendro)
library(ggplot2)
library(gridExtra)
library(heatmaply)
library(plotly)
library(purrr)
library(RColorBrewer)
library(rpart)
library(rpart.plot)
library(spotifyr)
library(tidymodels)
library(tidyverse)
library(yardstick)

# Get playlists
songs_i_like <- get_playlist_audio_features("", "6pl0C7qbIl5uoY3Tdf82oa")

songs_i_like <- songs_i_like %>%
  mutate(mode = ifelse(mode == 0, "Minor", "Major"))

songs_i_dislike <- get_playlist_audio_features("", "4bJQX5w7W4wEnHLmWqUIVY")

songs_i_dislike <- songs_i_dislike %>%
  mutate(mode = ifelse(mode == 0, "Minor", "Major"))

# Define a custom color palette
random_palette <- colorRampPalette(c("#2c7bb6", "#abdda4", "#ffffbf", "#fdae61", "#d7191c"))(100)
blue_red <- colorRampPalette(c("#00008B", "#ADD8E6", "#E6FFFF", "#FFE4E1", "#FF6347", "#8B0000"))(100)
```

```{r chords and keys setup, include=FALSE}
circshift <- function(v, n) {
  if (n == 0) v else c(tail(v, n), head(v, -n))
}

#      C     C#    D     Eb    E     F     F#    G     Ab    A     Bb    B
major_chord <-
  c(   1,    0,    0,    0,    1,    0,    0,    1,    0,    0,    0,    0)
minor_chord <-
  c(   1,    0,    0,    1,    0,    0,    0,    1,    0,    0,    0,    0)
seventh_chord <-
  c(   1,    0,    0,    0,    1,    0,    0,    1,    0,    0,    1,    0)

major_key <-
  c(6.35, 2.23, 3.48, 2.33, 4.38, 4.09, 2.52, 5.19, 2.39, 3.66, 2.29, 2.88)
minor_key <-
  c(6.33, 2.68, 3.52, 5.38, 2.60, 3.53, 2.54, 4.75, 3.98, 2.69, 3.34, 3.17)

chord_templates <-
  tribble(
    ~name, ~template,
    "Gb:7", circshift(seventh_chord, 6),
    "Gb:maj", circshift(major_chord, 6),
    "Bb:min", circshift(minor_chord, 10),
    "Db:maj", circshift(major_chord, 1),
    "F:min", circshift(minor_chord, 5),
    "Ab:7", circshift(seventh_chord, 8),
    "Ab:maj", circshift(major_chord, 8),
    "C:min", circshift(minor_chord, 0),
    "Eb:7", circshift(seventh_chord, 3),
    "Eb:maj", circshift(major_chord, 3),
    "G:min", circshift(minor_chord, 7),
    "Bb:7", circshift(seventh_chord, 10),
    "Bb:maj", circshift(major_chord, 10),
    "D:min", circshift(minor_chord, 2),
    "F:7", circshift(seventh_chord, 5),
    "F:maj", circshift(major_chord, 5),
    "A:min", circshift(minor_chord, 9),
    "C:7", circshift(seventh_chord, 0),
    "C:maj", circshift(major_chord, 0),
    "E:min", circshift(minor_chord, 4),
    "G:7", circshift(seventh_chord, 7),
    "G:maj", circshift(major_chord, 7),
    "B:min", circshift(minor_chord, 11),
    "D:7", circshift(seventh_chord, 2),
    "D:maj", circshift(major_chord, 2),
    "F#:min", circshift(minor_chord, 6),
    "A:7", circshift(seventh_chord, 9),
    "A:maj", circshift(major_chord, 9),
    "C#:min", circshift(minor_chord, 1),
    "E:7", circshift(seventh_chord, 4),
    "E:maj", circshift(major_chord, 4),
    "G#:min", circshift(minor_chord, 8),
    "B:7", circshift(seventh_chord, 11),
    "B:maj", circshift(major_chord, 11),
    "D#:min", circshift(minor_chord, 3)
  )

key_templates <-
  tribble(
    ~name, ~template,
    "Gb:maj", circshift(major_key, 6),
    "Bb:min", circshift(minor_key, 10),
    "Db:maj", circshift(major_key, 1),
    "F:min", circshift(minor_key, 5),
    "Ab:maj", circshift(major_key, 8),
    "C:min", circshift(minor_key, 0),
    "Eb:maj", circshift(major_key, 3),
    "G:min", circshift(minor_key, 7),
    "Bb:maj", circshift(major_key, 10),
    "D:min", circshift(minor_key, 2),
    "F:maj", circshift(major_key, 5),
    "A:min", circshift(minor_key, 9),
    "C:maj", circshift(major_key, 0),
    "E:min", circshift(minor_key, 4),
    "G:maj", circshift(major_key, 7),
    "B:min", circshift(minor_key, 11),
    "D:maj", circshift(major_key, 2),
    "F#:min", circshift(minor_key, 6),
    "A:maj", circshift(major_key, 9),
    "C#:min", circshift(minor_key, 1),
    "E:maj", circshift(major_key, 4),
    "G#:min", circshift(minor_key, 8),
    "B:maj", circshift(major_key, 11),
    "D#:min", circshift(minor_key, 3)
  )

```

Introduction {data-icon="fa-spotify"}
=====================================

Column {data-width=550}
-----------------------------------------------------------------------

###

**Introduction:**

Music is an essential part of many people's lives, and with digital music streaming platforms like Spotify, discovering new music has become more accessible than ever. Spotify's algorithm suggests playlists based on users' listening habits and preferences. One of the most popular playlists on Spotify is "New Music Friday," updated every week with new releases from various artists. It is curated by Spotify's editorial team, who select the latest and most popular releases from various genres. However, the question remains: how accurately does this playlist align with personal musical taste and style?

As a student and an avid music lover, I have always been curious about the mechanics behind music recommendations. This final portfolio explores the inner workings of Spotify's music recommendation algorithm, attempting to create a predictive model that accurately predicts a user's preferred musical genres and styles. The analysis will focus on two playlists - "Songs I Like" and "Songs I Dislike" - to identify patterns in personal musical taste. The "Songs I Like" playlist is a collection of songs that hold personal meaning, representing various genres such as pop, 70s, 80s, 90s musical, and rock. In contrast, the "Songs I Dislike" playlist consists of heavy metal and drill rap songs that do not match personal taste.

By analysing the features of songs in both playlists, this portfolio aims to provide insights into personal musical preferences and use this information to train a machine learning model. The goal is to create a predictive model that accurately predicts preferred musical genres and styles. Additionally, the analysis will investigate whether this model can predict which songs from popular playlists like "New Music Friday" would resonate with the user, allowing for a more personalized and tailored listening experience.

**Background:**

The tracks chosen for this portfolio come from a personal selection of songs representing the music I enjoy and the genres I tend to avoid. This decision was made to provide a diverse and authentic dataset for the machine learning model. By examining these songs, the portfolio seeks to answer the following questions:

1. What are the key features that differentiate songs from the "Songs I Like" and "Songs I Dislike" playlists?
2. Can a machine learning model be trained to predict personal musical preferences based on these features?
3. How effectively can the trained model predict which songs from popular playlists, such as "New Music Friday," will align with my personal taste?


Column {data-width=225}
-----------------------------------------------------------------------

###

<iframe src="https://open.spotify.com/embed/playlist/6pl0C7qbIl5uoY3Tdf82oa?utm_source=generator" width="100%" height="100%" frameBorder="0" allowfullscreen="" allow="autoplay; clipboard-write; encrypted-media; fullscreen; picture-in-picture"></iframe>


Column {data-width=225}
-----------------------------------------------------------------------

###

<iframe src="https://open.spotify.com/embed/playlist/4bJQX5w7W4wEnHLmWqUIVY?utm_source=generator&theme=0" width="100%" height="100%" frameBorder="0" allowfullscreen="" allow="autoplay; clipboard-write; encrypted-media; fullscreen; picture-in-picture"></iframe>

Feature analysis {.storyboard data-icon="fa-chart-bar"}
=====================================

### Relationship between Valence, Energy, Loudness, and Mode for the songs I like 

```{r songs_i_like plot}
# Create a scatterplot
songs_i_like %>%
  ggplot(aes(x = valence, y = energy, size = loudness, color = mode)) +
  geom_point(alpha = 0.5) +
  scale_x_continuous(limits = c(0, 1), breaks = c(0, 0.50, 1), minor_breaks = NULL) +
  scale_y_continuous(limits = c(0, 1), breaks = c(0, 0.50, 1), minor_breaks = NULL) +
  scale_color_manual(values = c("darkblue", "lightblue"), name = "Mode", labels = c("Minor", "Major")) +
  scale_size_continuous(trans = "exp", range = c(1, 10), guide = guide_legend(override.aes = list(size = c(1, 3, 5)))) +
  theme_light() +
  labs(x = "Valence", y = "Energy",
       title = "Songs I like",
       subtitle = "Plotting valence, energy, and loudness")
```

---

Spotify's track-level features, such as valence, danceability, energy, and loudness, offer valuable insights into the underlying patterns of my personal music preferences. I selected these features as they are commonly used to describe the overall mood, intensity, and emotional tone of a song, allowing us to better understand the characteristics that define the songs I like and dislike [(Serra et al.)](https://pubmed.ncbi.nlm.nih.gov/22837813/). By examining these features, we can gain a deeper understanding of the traits that influence my musical taste.

Based on the first scatterplot, it seems like I tend to like songs that are loud. I also seem to be drawn to songs that are high-energy, as they're clustered towards the upper portion of the plot where the energy values are high.

As for the valence of the songs I like, it appears to be centered around 0.5, but mostly falls between 0.5 and 1.0. This suggests that I tend to prefer songs with a positive emotional valence, which could contribute to their appeal.

One interesting thing I noticed in the plot is that the songs I like are equally distributed between major and minor keys, as indicated by the color coding in the plot. This suggests that the mode of the songs I like doesn't strongly influence my preference for them.

### Relationship between Valence, Energy, Loudness, and Mode for the songs I dislike

```{r songs_i_dislike plot}
# Create a scatterplot
songs_i_dislike %>%
  ggplot(aes(x = valence, y = energy, size = loudness, color = mode)) +
  geom_point(alpha = 0.5) +
  scale_x_continuous(limits = c(0, 1), breaks = c(0, 0.50, 1), minor_breaks = NULL) +
  scale_y_continuous(limits = c(0, 1), breaks = c(0, 0.50, 1), minor_breaks = NULL) +
  scale_color_manual(values = c("darkred", "lightcoral")) +
  scale_size_continuous(trans = "exp", range = c(1, 10), guide = guide_legend(override.aes = list(size = c(1, 3, 5, 7)))) +
  theme_light() +
  labs(x = "Valence", y = "Energy",
       title = "Songs I dislike",
       subtitle = "Plotting valence, energy, loudness, and mode")
```

---

Looking at the scatterplot of songs I don't like, I can see that they're usually not as loud as the songs I enjoy. Even though they still have high energy levels, they often have lower valence. This makes me believe that I might like songs that have a more positive and uplifting vibe, rather than ones that are more downbeat or sad.

It's interesting to see that, just like the songs I like, the songs I don't like are spread out pretty evenly between major and minor keys. This tells me that the key of a song doesn't really play a big role in whether I like it or not.

Another thing I noticed is that the songs I don't like seem to have a smaller range of energy compared to the songs I do like. This could mean that I'm more open to different levels of intensity and dynamics when it comes to music I enjoy, while the music I don't like might have more in common in terms of how energetic they are.


### Zooming in on Valence

```{r valence_plot}
plot_ly() %>%
  add_trace(data = songs_i_like, x = ~valence, type = "histogram", 
            histnorm = "probabilty density", name = "Songs I like", 
            marker = list(color = "lightblue", opacity = 0.5)) %>%
  add_trace(data = songs_i_dislike, x = ~valence, type = "histogram", 
            histnorm = "probabilty density", name = "Songs I dislike", 
            marker = list(color = "lightcoral", opacity = 0.5)) %>%
  layout(xaxis = list(title = "Valence"), yaxis = list(title = "Density"),
         title = "Valence Distribution",
         showlegend = TRUE,
         legend = list(title = "", orientation = "h"))
```

---

This plot provides a visual representation of how valence is distributed among two different sets of songs: 'Songs I like' and 'Songs I dislike'.

Valence is a measure of how positive or negative a musical piece sounds, with a scale that ranges from negative to positive values. The plot shows two histograms, one in light blue representing the valence distribution for the songs that I like, and the other in light coral representing the valence distribution for the songs that I dislike.

By looking at the plot, we can see that the valence distribution for the songs that I like is skewed towards the positive end of the spectrum, indicating that the songs the I like tend to have a more positive sound. In contrast, the valence distribution for the songs that I dislike is more skewed towards the negative end of the spectrum, suggesting that the songs I dislike have a more negative sound.

### Zooming in on Energy

```{r energy_plot}
plot_ly() %>%
  add_trace(data = songs_i_like, x = ~energy, type = "histogram", 
            histnorm = "probabilty density", name = "Songs I like", 
            marker = list(color = "lightgreen", opacity = 0.5)) %>%
  add_trace(data = songs_i_dislike, x = ~energy, type = "histogram", 
            histnorm = "probabilty density", name = "Songs I dislike", 
            marker = list(color = "coral", opacity = 0.5)) %>%
  layout(xaxis = list(title = "Energy"), yaxis = list(title = "Density"),
         title = "Energy Distribution",
         showlegend = TRUE,
         legend = list(title = "", orientation = "h"))
```

---

This plot provides insight into the energy distribution of two different sets of songs: 'Songs I like' and 'Songs I dislike'. Energy is a measure of how intense and active a musical piece is, with a scale that ranges from low to high values. The plot displays two histograms, one in light green representing the energy distribution for the songs that I like, and the other in coral representing the energy distribution for the songs that I dislike.

Upon examining the plot, we can see that the energy distribution for the songs that I like is concentrated between 0.7 and 0.9, with a peak around 0.8. This indicates that the songs that I enjoy tend to have a moderate level of energy, without being too intense or too mellow. In contrast, the energy distribution for the songs that I dislike is concentrated around 0.95, indicating that the songs that I find unfavorable tend to be more energetic.

The findings of this plot suggest that energy may be an important factor in shaping my musical preferences. However, it is essential to note that energy is just one of many factors that can influence an individual's musical tastes.

### Zooming in on Loudness

```{r loudness_plot}
plot_ly() %>%
  add_trace(data = songs_i_like, x = ~loudness, type = "histogram", 
            histnorm = "probabilty density", name = "Songs I like", 
            marker = list(color = "#00BFC4", opacity = 0.5)) %>%
  add_trace(data = songs_i_dislike, x = ~loudness, type = "histogram", 
            histnorm = "probabilty density", name = "Songs I dislike", 
            marker = list(color = "#F8766D", opacity = 0.5))%>%
  layout(xaxis = list(title = "Loudness"), yaxis = list(title = "Density"),
         title = "Loudness Distribution",
         showlegend = TRUE,
         legend = list(title = "", orientation = "h"))
```

---

Musical preferences are complex and can be influenced by various factors, including the loudness of a musical piece. Loudness is an essential characteristic of music that can affect how it is perceived by the listener. It refers to the volume or intensity of a musical piece and is typically measured in decibels.

The plot that highlights the difference in loudness between songs that I like and songs that I dislike may provide insight into how loudness influences my musical preferences. The plot may indicate that my preferred songs tend to be louder than songs that I dislike, suggesting that loudness is an essential characteristic of my musical taste. However, it is crucial to remember that loudness is just one factor that can influence musical preferences.

It is worth noting that loudness is not always a reliable indicator of a songs quality or emotional impact. A quiet ballad or a soothing instrumental piece can be just as powerful and moving as a loud and energetic track. Therefore, while loudness may be an important characteristic of my preferred songs, it should not be the only factor considered when evaluating the quality or emotional impact of a musical piece.

Track-Level Summary {data-icon="fa-diagram-venn"}
=====================================

Overview
--------------------------------------------------

###

In any portfolio, selecting appropriate samples is crucial in effectively achieving the portfolio objectives. In the present portfolio, a broad range of samples was chosen, including diverse data related to the research question. However, the large volume of data made it challenging to identify significant patterns or trends that could adequately address the research question.

To overcome this challenge, the decision was made to focus on the outliers in the portfolio. Specifically, the analysis focused on the extreme cases that were most divergent from the norm in terms of a specific component. This approach allowed for the isolation and study of the outliers, leading to valuable insights and a better understanding of the factors contributing to their unique characteristics. Ultimately, this approach strengthened the analysis and enhanced the quality of the portfolio.

In this portfolio, timbre, the quality of sound that distinguishes different musical instruments, was analyzed using spectral content, which measures the relative strengths of various frequency components that make up the sound. The portfolio focused on a specific timbre component, which was used to isolate and study the outliers in the samples.

To further investigate this approach, tables were generated to identify the maximum and minimum values of specific timbre components for each sample. The tables for the maximum and minimum values of c01, c02, c03, and c04 for both the 'Songs I like' and 'Songs I dislike' samples showed notable differences in timbre components between the two sets of samples. For example, the table of maximum and minimum values of c02 for each set of samples showed that the highest and lowest timbre values for 'Songs I like' were exhibited by the samples ['What I Like About You'](https://open.spotify.com/track/4ebcE2SmkG7nplvzFAWRu7?si=c432fccad51845ae), and ['The Sailor’s Warning'](https://open.spotify.com/track/3bgmoyCHa6v3bMMsgK1rWh?si=d7b40ddaef5344a1), respectively, while the highest and lowest timbre values for 'Songs I dislike' were exhibited by the samples ['Murder'](https://open.spotify.com/track/0wFPKFtBes3NTyTW4bkjrz?si=796b185ece5d4a75) and ['19 Tini 5'](https://open.spotify.com/track/73myy2XXpvC2bHi4Fikksr?si=775791f95d43461a), respectively., respectively.

These tables provide insight into how specific timbre components differ between the two sets of samples, which helps to support the approach of focusing on the outliers in the portfolio to gain valuable insights into timbre characteristics.

Column 2 {.tabset}
--------------------------------------------------

### Timbre components

```{r timbre_summary}
timbre_data <- readRDS(file="data/timbre_data.Rda")

plot1 <- timbre_data %>%
  ggplot(aes(x = factor(basis), y = value, fill = Playlist)) +
  geom_violin() +
  scale_fill_viridis_d() +
  labs(x = "Spotify Timbre Coefficients", y = "", fill = "Playlist") +
  theme_minimal()

ggplotly(plot1)
```

### c01

```{r c01}
# Create table for c01
table_c01 <- timbre_data %>%
  filter(basis == "c01") %>%
  group_by(Playlist) %>%
  slice_min(value, n = 1) %>%
  bind_rows(timbre_data %>%
              filter(basis == "c01") %>%
              group_by(Playlist) %>%
              slice_max(value, n = 1)) %>%
  select(Playlist, track.name, artists, value) %>%
  rename("Track" = "track.name", "Artists" = "artists")

knitr::kable(table_c01, caption = "Table of maximum and minimum values of c01 for each playlist")

```

### c02

```{r c02}
# Create table for c02
table_c02 <- timbre_data %>%
  filter(basis == "c02") %>%
  group_by(Playlist) %>%
  slice_min(value, n = 1) %>%
  bind_rows(timbre_data %>%
              filter(basis == "c02") %>%
              group_by(Playlist) %>%
              slice_max(value, n = 1)) %>%
  select(Playlist, track.name, artists, value) %>%
  rename("Track" = "track.name", "Artists" = "artists")

knitr::kable(table_c02, caption = "Table of maximum and minimum values of c02 for each playlist")
```

### c03

```{r c03}
# Create table for c03
table_c03 <- timbre_data %>%
  filter(basis == "c03") %>%
  group_by(Playlist) %>%
  slice_min(value, n = 1) %>%
  bind_rows(timbre_data %>%
              filter(basis == "c03") %>%
              group_by(Playlist) %>%
              slice_max(value, n = 1)) %>%
  select(Playlist, track.name, artists, value) %>%
  rename("Track" = "track.name", "Artists" = "artists")

knitr::kable(table_c03, caption = "Table of maximum and minimum values of c03 for each playlist")
```

### c04

```{r c04}
# Create table for c04
table_c04 <- timbre_data %>%
  filter(basis == "c04") %>%
  group_by(Playlist) %>%
  slice_min(value, n = 1) %>%
  bind_rows(timbre_data %>%
              filter(basis == "c04") %>%
              group_by(Playlist) %>%
              slice_max(value, n = 1)) %>%
  select(Playlist, track.name, artists, value) %>%
  rename("Track" = "track.name", "Artists" = "artists")

knitr::kable(table_c04, caption = "Table of maximum and minimum values of c04 for each playlist")
```


Chromagrams {data-icon="fa-music"}
=====================================

Text about chromagrams {data-width=400}
--------------------------------------------------

###

Chromagrams are powerful tools for analyzing the harmonic content and structural features of music. In this portfolio, we examined two songs from my musical corpus: "What I Like About You" from the playlist 'Songs I Like' and "Murder" from the playlist 'Songs I Dislike'. These songs were analyzed using their respective chromagrams to gain insight into their harmonic structures and to better understand my personal musical preferences.

["What I Like About You"](https://open.spotify.com/track/4ebcE2SmkG7nplvzFAWRu7?si=c432fccad51845ae) by The Romantics has a simple harmonic structure that consists of the chords A, G#, E, and D. The corresponding chromagram for this song shows a clear and consistent distribution of these pitches. This simple harmonic structure may contribute to why I enjoy this song and have included it in my 'Songs I Like' playlist.

On the other hand, ["Murder"](https://open.spotify.com/track/0wFPKFtBes3NTyTW4bkjrz?si=796b185ece5d4a75) has a more complex harmonic structure that includes frequent modulations but tends to avoid the pitches C# and C. The chromagram for this song displays a diverse distribution of pitches, indicating a more intricate harmonic structure and chord progression. This variation in pitch distribution may be the reason why I dislike this song and have included it in my 'Songs I Dislike' playlist.

By comparing the chromagrams of these two songs, we can gain insights into the underlying harmonic structures that contribute to my personal musical taste. The analysis of chromagrams allows us to identify commonalities and differences between songs, which can be used to curate playlists that align with our individual musical preferences.


Chromagrams {data-width=600}
--------------------------------------------------

###

```{r chromagram1}
wilay <-
  get_tidy_audio_analysis("4ebcE2SmkG7nplvzFAWRu7") |>
  select(segments) |>
  unnest(segments) |>
  select(start, duration, pitches)

chromogram1 <- wilay |>
  mutate(pitches = map(pitches, compmus_normalise, "euclidean")) |>
  compmus_gather_chroma() |> 
  ggplot(
    aes(
      x = start + duration / 2,
      width = duration,
      y = pitch_class,
      fill = value
    )
  ) +
  geom_tile() +
  ggtitle("Chromagram - The Romantics: 'What I Like About You'") +
  scale_fill_gradientn(colors = random_palette) +
  labs(x = "Time (s)", y = NULL, fill = "Magnitude") +
  theme_minimal() +
  scale_color_gradientn(colors = random_palette)

ggplotly(chromogram1, tooltip = "none", source = "none")
```

###

```{r chromagram 2}
murder <-
  get_tidy_audio_analysis("0wFPKFtBes3NTyTW4bkjrz") |>
  select(segments) |>
  unnest(segments) |>
  select(start, duration, pitches)

chromogram2 <- murder |>
  mutate(pitches = map(pitches, compmus_normalise, "euclidean")) |>
  compmus_gather_chroma() |> 
  ggplot(
    aes(
      x = start + duration / 2,
      width = duration,
      y = pitch_class,
      fill = value
    )
  ) +
  geom_tile() +
  ggtitle("Chromagram - DJ Squeeky, Tom Skeemask, GK: 'Murder'") +
  scale_fill_gradientn(colors = random_palette) +
  labs(x = "Time (s)", y = NULL, fill = "Magnitude") +
  theme_minimal() +
  scale_color_gradientn(colors = random_palette)

ggplotly(chromogram2, tooltip = "none", source = "none")
```

Ceptrograms {data-icon="fa-headphones-alt"}
=====================================

Text about ceptrograms {data-width=400}
--------------------------------------------------

###

Ceptrograms are an essential tool for analyzing the timbral characteristics of a musical recording. A ceptrogram provides a visual representation of the distribution of timbral characteristics such as brightness, warmth, and depth in a song. By comparing the ceptrograms of two songs, we can gain insights into the similarities and differences in their timbral characteristics and use this knowledge to better understand our personal musical preferences.

In this portfolio, we analyzed two songs from my corpus, ["Shut Up and Dance"](https://open.spotify.com/track/4kbj5MwxO1bq9wjT5g9HaA?si=caf4a51f4d4645c4) from the 'Songs I Like' playlist and ["State of Unrest"](https://open.spotify.com/track/3u4djE2yAEkKMWJEUOOJyT?si=1230a654e318462a) from the 'Songs I Dislike' playlist, using their respective ceptrograms to understand their timbral structures and to gain insights into why I might have included them in their respective playlists.

Upon analyzing the ceptrogram of "Shut Up and Dance," we observed a strong presence around the c01 and c02 timbre coefficients. These coefficients indicate that the song has a bright and loud timbral quality, which may contribute to why I enjoy listening to it and why it is included in my 'Songs I Like' playlist.

On the other hand, "State of Unrest" exhibits a similar distribution of timbral characteristics as "Shut Up and Dance," with a strong presence around the c01 and c02 timbre coefficients. However, this song is included in my 'Songs I Dislike' playlist, indicating that timbre is not the main factor that determines my musical preferences.

In conclusion, the analysis of ceptrograms for the songs "Shut Up and Dance" and "State of Unrest" provides us with insights into their timbral structures and how they may contribute to our personal musical preferences. While "Shut Up and Dance" exhibits a bright and loud timbral quality that may explain why it is included in the 'Songs I Like' playlist, "State of Unrest" has a similar distribution of timbral characteristics but is included in the 'Songs I Dislike' playlist, indicating that other factors beyond timbre play a role in determining our musical preferences.


Ceptrograms {data-width=600}
--------------------------------------------------

###

```{r ceptrogram 1}
state_of_unrest <- readRDS(file="data/Ceptrogram (state_of_unrest).Rda")

ceptrogram1 <- state_of_unrest |>
  ggplot(aes(x = start + duration / 2, width = duration, y = basis, fill = value)) +
  geom_tile() +
  ggtitle("Ceptrogram - Lamb of God, Kreator: 'State of Unrest'") +
  scale_fill_gradientn(colors = random_palette) +
  labs(x = "Time (s)", y = NULL, fill = "Magnitude") +
  theme_minimal() +
  scale_color_gradientn(colors = random_palette)

ggplotly(ceptrogram1, tooltip = "none", source = "none")
```

###

```{r ceptrogram 2}
shutupanddance <- readRDS(file="data/Ceptrogram (shutupanddance).Rda")

ceptrogram2 <- shutupanddance |>
  ggplot(aes(x = start + duration / 2, width = duration, y = basis, fill = value)) +
  geom_tile() +
  ggtitle("Ceptrogram - WALK THE MOON: 'Shut up and Dance'") +
  scale_fill_gradientn(colors = random_palette) +
  labs(x = "Time (s)", y = NULL, fill = "Magnitude") +
  theme_minimal() +
  scale_color_gradientn(colors = random_palette)

ggplotly(ceptrogram2, tooltip = "none", source = "none")
```


Tempograms {data-icon="fas fa-drum"}
=====================================

Text about tempograms {data-width=400}
--------------------------------------------------

###

Tempo, which refers to the speed or pace of a piece of music, is an essential aspect that greatly influences listeners' perception and emotional response to music. Some songs have a slow and steady tempo that can evoke feelings of calmness and relaxation, while others have a fast and upbeat tempo that may evoke feelings of excitement and energy. Tempograms are visual representations of the tempo fluctuations in a piece of music over time. They provide a useful tool for analyzing and comparing the tempo patterns of different songs, allowing us to gain insights into the structure and dynamics of musical compositions.

To explore the range of tempos in the 'Songs I like' playlist, I have selected two songs with the highest and lowest tempos, namely ['High Hopes'](https://open.spotify.com/track/1rqqCSm0Qe4I9rUvWncaom?si=14d92d54789c4081) by Panic! At The Disco with a tempo of 82.014 and ['Fascination'](https://open.spotify.com/track/5AKQ1JHezaXDmN5SyMSpEr?si=6478428cf0cf45a7) by Alphabeat with a tempo of 193.968. These songs offer a glimpse into the variety of tempos found in the playlist and provide an opportunity to observe any tempo changes in slow and fast songs.

Upon analyzing the Tempograms, it can be observed that both songs maintain a relatively consistent tempo with occasional variations. 'High Hopes' mostly maintains a tempo of around 80 BMP, while 'Fascination' hovers around 98 BPM. However, 'High Hopes' displays more frequent tempo changes compared to 'Fascination.'

In conclusion, the selected songs from the 'Songs I like' playlist offer insights into the range of tempos in music that I enjoy. The Tempogram analysis has also revealed interesting patterns in tempo fluctuations, which can further enhance our understanding of the role of tempo in music taste.


Tempograms {data-width=600}
--------------------------------------------------

### 

```{r tempogram_1}
high_hopes_cyclic <- readRDS(file="data/Tempogram (high_hopes_cyclic).Rda")

tempogram_1 <- high_hopes_cyclic |>
  ggplot(aes(x = time, y = bpm, fill = power)) +
  geom_raster() +
  ggtitle("Cyclic Tempogram - Panic! At The Disco: 'High Hopes'") +
  scale_fill_gradientn(colors = random_palette) +
  labs(x = "Time (s)", y = "Tempo (BPM)") +
  theme_minimal() +
  scale_color_gradientn(colors = random_palette)

ggplotly(tempogram_1, tooltip = "none", source = "none")
```

### 

```{r tempogram_2}
fascination_cyclic <- readRDS(file="data/Tempogram (fascination_cyclic).Rda")

tempogram_2 <- fascination_cyclic |>
  ggplot(aes(x = time, y = bpm, fill = power)) +
  geom_raster() +
  ggtitle("Cyclic Tempogram - Alphabeat: 'Fascination'") +
  scale_fill_gradientn(colors = random_palette) +
  labs(x = "Time (s)", y = "Tempo (BPM)") +
  theme_minimal() +
  scale_color_gradientn(colors = random_palette)

ggplotly(tempogram_2, tooltip = "none", source = "none")
```

Self-similarity Matrices {data-icon="fas fa-angle-double-right"}
=====================================

text over SSM {data-width=500}
--------------------------------------------------

###

A self-similarity matrix is a visual representation of the similarity between different sections of a musical recording. This tool allows us to observe and analyze the internal structure of a song in a way that provides valuable insights into its musical composition and arrangement. By comparing the self-similarity matrices of two songs, one I like a lot and one I dislike, ["Shut Up and Dance"](https://open.spotify.com/track/4kbj5MwxO1bq9wjT5g9HaA?si=caf4a51f4d4645c4) and ["State of Unrest"](https://open.spotify.com/track/3u4djE2yAEkKMWJEUOOJyT?si=1230a654e318462a) in both timbre and chroma, we can gain a deeper understanding of how these songs are structured and how they differ from each other.

When analyzing the self-similarity matrix for the timbral characteristics of the songs, we can see variations in the repetition of certain musical motifs. These motifs could be rhythmic patterns, melodies, or harmonies that repeat throughout the song, providing a sense of continuity and coherence. In the case of "State of Unrest" and "Shut Up and Dance," we can see that the two songs differ in terms of the frequency and regularity of these motifs. "Shut Up and Dance" has a more consistent and structured pattern of repetition, while "State of Unrest" has a less predictable and more sporadic one.

Similarly, the chroma self-similarity matrix provides insights into how the songs' chord progressions and tonal centers change over time. This information is particularly valuable in understanding the harmonic structure of a song, which is essential in determining its overall mood and emotional impact. In the case of "State of Unrest" and "Shut Up and Dance," we can see that the two songs differ in terms of their harmonic complexity and diversity. "Shut Up and Dance" has a more straightforward and predictable harmonic structure, while "State of Unrest" explores a wider range of harmonic possibilities.

One notable difference between the two songs is that, around the 150-second mark, the bridge section in "Shut Up and Dance" is clearly visible in both timbre and chroma self-similarity matrices, while in "State of Unrest," this section is less structured and organized. 

Through this analysis, I can see that "Shut Up and Dance" follows a more traditional song structure, with a clear and predictable form that includes defined verses, choruses, and bridges. In contrast, "State of Unrest" may have a more unconventional arrangement, with a less predictable and more sporadic pattern of repetition. While both songs have their unique qualities, I tend to respond positively to songs that have a clear and familiar structure, and this could be one of the reasons why I find "Shut Up and Dance" more appealing than "State of Unrest."

SSM {data-width=250}
--------------------------------------------------

###

```{r compmus_self_similarity1}
state_of_unrest_chroma <- readRDS("data/Self-similarity Matrix (state_of_unrest_chroma).Rda")

ssm1 <- state_of_unrest_chroma |> 
  ggplot(
    aes(
      x = xstart + xduration / 2,
      width = xduration,
      y = ystart + yduration / 2,
      height = yduration,
      fill = d
    )
  ) +
  geom_tile() +
  ggtitle("Self-similarity Matrix (Chroma)\nLamb of God, Kreator: 'State of Unrest'") +
  scale_fill_gradientn(colors = blue_red) +
  labs(x = NULL, y = NULL, fill = "") +
  theme_minimal() +
  scale_color_gradientn(colors = blue_red) +
  theme(plot.title = element_text(size = 10))

ggplotly(ssm1, tooltip = "none", source = "none")
```


###

```{r compmus_self_similarity2}
state_of_unrest_timbre <- readRDS("data/Self-similarity Matrix (state_of_unrest_timbre).Rda")

ssm2 <- state_of_unrest_timbre |> 
  ggplot(
    aes(
      x = xstart + xduration / 2,
      width = xduration,
      y = ystart + yduration / 2,
      height = yduration,
      fill = d
    )
  ) +
  geom_tile() +
  ggtitle("Self-similarity Matrix (Timbre)\nLamb of God, Kreator: 'State of Unrest'") +
  scale_fill_gradientn(colors = blue_red) +
  labs(x = NULL, y = NULL, fill = "") +
  theme_minimal() +
  scale_color_gradientn(colors = blue_red) +
  theme(plot.title = element_text(size = 10))

ggplotly(ssm2, tooltip = "none", source = "none")
```

column3 {data-width=250}
--------------------------------------------------

###

```{r compmus_self_similarity3}
shutupanddance_chroma <- readRDS("data/Self-similarity Matrix (shutupanddance_chroma).Rda")

ssm3 <- shutupanddance_chroma |> 
  ggplot(
    aes(
      x = xstart + xduration / 2,
      width = xduration,
      y = ystart + yduration / 2,
      height = yduration,
      fill = d
    )
  ) +
  geom_tile() +
  ggtitle("Self-similarity Matrix (Chroma)\nWALK THE MOON: 'Shut up and Dance'") +
  scale_fill_gradientn(colors = blue_red) +
  labs(x = NULL, y = NULL, fill = "") +
  theme_minimal() +
  scale_color_gradientn(colors = blue_red) +
  theme(plot.title = element_text(size = 10))

ggplotly(ssm3, tooltip = "none", source = "none")
```

###

```{r compmus_self_similarity4}
shutupanddance_timbre <- readRDS("data/Self-similarity Matrix (shutupanddance_timbre).Rda")

ssm4 <- shutupanddance_timbre |> 
  ggplot(
    aes(
      x = xstart + xduration / 2,
      width = xduration,
      y = ystart + yduration / 2,
      height = yduration,
      fill = d
    )
  ) +
  geom_tile() +
  ggtitle("Self-similarity Matrix (Timbre)\nWALK THE MOON: 'Shut up and Dance'") +
  scale_fill_gradientn(colors = blue_red) +
  labs(x = NULL, y = NULL, fill = "") +
  theme_minimal() +
  scale_color_gradientn(colors = blue_red) +
  theme(plot.title = element_text(size = 10))

ggplotly(ssm4, tooltip = "none", source = "none")
```


Trained Model {.storyboard data-icon="fa-solid fa-network-wired"}
=====================================

### Unlocking the secrets of my playlist: A decision tree classification model investigation

```{r display the decision tree, fig.width = 8, fig.height = 5, out.width = "100%", out.height = "100%"}
get_conf_mat <- function(fit) {
  outcome <- .get_tune_outcome_names(fit)
  fit |> 
    collect_predictions() |> 
    conf_mat(truth = outcome, estimate = .pred_class)
}  

get_pr <- function(fit) {
  fit |> 
    conf_mat_resampled() |> 
    group_by(Prediction) |> mutate(precision = Freq / sum(Freq)) |> 
    group_by(Truth) |> mutate(recall = Freq / sum(Freq)) |> 
    ungroup() |> filter(Prediction == Truth) |> 
    select(class = Prediction, precision, recall)
}  

get_pr2 <- function(cm) {
  true_positives <- diag(cm)
  false_positives <- colSums(cm) - true_positives
  false_negatives <- rowSums(cm) - true_positives
  precision <- true_positives / (true_positives + false_positives)
  recall <- true_positives / (true_positives + false_negatives)
  class_labels <- rownames(cm)
  tibble(class = class_labels, precision = precision, recall = recall)
}

# see make_model.R
new_music_friday <- readRDS(file="data/Model (new_music_friday).Rda")
new_music_friday_features <- readRDS(file="data/Model (new_music_friday_features).Rda")
corpus <- readRDS(file="data/Model (corpus).Rda")
corpus_features <- readRDS(file="data/Model (corpus_features).Rda")

corpus_recipe <-
  recipe(
    playlist ~
      danceability +
      energy +
      loudness +
      speechiness +
      acousticness +
      instrumentalness +
      liveness +
      valence +
      tempo +
      duration,
    data = corpus_features           # Use the same name as the previous block.
  ) |>
  step_center(all_predictors()) |>
  step_scale(all_predictors()) |>    # Converts to z-scores.
  step_range(all_predictors())       # Sets range to [0, 1].

new_music_friday_recipe <-
  recipe(
    playlist ~
      danceability +
      energy +
      loudness +
      speechiness +
      acousticness +
      instrumentalness +
      liveness +
      valence +
      tempo +
      duration,
    data = new_music_friday_features # Use the same name as the previous block.
  ) |>
  step_center(all_predictors()) |>
  step_scale(all_predictors()) |>    # Converts to z-scores.
  step_range(all_predictors())       # Sets range to [0, 1].

corpus_cv <- corpus_features |> vfold_cv(5)

# Create a decision tree model
decision_tree_model <-
  decision_tree(tree_depth = 3, min_n = 25) |> 
  set_mode("classification") |> 
  set_engine("rpart")

# Fit the decision tree model with cross-validation
corpus_decision_tree <- 
  workflow() |> 
  add_recipe(corpus_recipe) |> 
  add_model(decision_tree_model) |> 
  fit_resamples(corpus_cv, control = control_resamples(save_pred = TRUE))

# Fit the decision tree model without cross-validation
corpus_decision_tree_no_cv <- 
  workflow() |> 
  add_recipe(corpus_recipe) |> 
  add_model(decision_tree_model) |> 
  fit(data = corpus_features)

# Extract the rpart object
rpart_obj <- corpus_decision_tree_no_cv %>% pull_workflow_fit() %>% pluck("fit")

# Plot the decision tree
rpart.plot(rpart_obj, type = 1, extra = 101, cex = 0.8, box.palette = "auto", tweak = 1.2, roundint = FALSE, main = "Decision tree trained on the corpus")
```

---

Finally, I trained a decision tree classification model on the two playlists ('Songs I like' and 'Songs I dislike'). I used all the features except for key and timbre components, as including them decreased recall and precision. During the analysis, I performed 5 cross-validations to ensure the model's accuracy. As can be seen in the plot, instrumentalness seems to be the most important feature, followed by speechiness. Finally, acousticness and energy are also important. Overall, it can be concluded that I like songs with low instrumentalness, low speechiness, and energy levels below 0.89, as shown in the feature analysis.

### Valuation of model performance using recall and precision metrics

```{r display the stats of the decision tree}
# Display confusion matrix as heatmap
heatmap1 <- corpus_decision_tree |> get_conf_mat() |>
  autoplot(type = "heatmap") + 
  scale_fill_gradientn(colors = brewer.pal(9, "Blues")) +
            ggtitle("Confusion Matrix of the model predicting\nlabels on the corpus")


heatmap1
```

---

After training the model, I evaluated its performance by applying it to the original dataset and calculating its recall and precision. Recall is the proportion of true positive predictions (i.e., correctly classified songs I like and songs I dislike) to the total number of actual positive cases, while precision is the proportion of true positive predictions to the total number of predicted positive cases.

The confusion matrix showed that the model performed best at classifying songs I like, but it also had a decent performance in classifying songs I dislike. Overall, the model's accuracy was fairly good, as demonstrated by the table below.

```{r stats}
# Get precision and recall
pr <- corpus_decision_tree |> get_pr()

# Display table of precision and recall
knitr::kable(pr, caption = "Precision and Recall of the Decision Tree classifying the corpus")
```

### Testing the model on the New Music Friday (NL) Spotify playlist to evaluate its ability to make accurate predictions based on my music preferences

```{r fit the decision tree to new music friday}
# Fit the decision tree model for new music friday
nmf_decision_tree <- 
  workflow() |> 
  add_recipe(new_music_friday_recipe) |> 
  add_model(decision_tree_model) |> 
  fit(data = new_music_friday_features)

# Get predictions from the model
nmf_predictions <- predict(nmf_decision_tree, new_music_friday_features) 

# Create a tibble with the true and predicted labels
nmf_pred <- tibble(
  truth = new_music_friday_features$playlist, 
  estimate = nmf_predictions$.pred_class
)

overview <- bind_cols(new_music_friday_features %>% 
                         select(`Track name` = track.name), 
                       `Labeled as` = nmf_pred$truth, 
                       `The model predicted` = nmf_pred$estimate)


knitr::kable(overview)
```

---

As a final test of my decision tree classification model, I wanted to see how it would perform on an unknown playlist. To achieve this, I hand-labeled the first 20 songs of the Spotify playlist 'New Music Friday (NL),' indicating which songs I would like and which ones I would dislike.

Next, I used the model to classify the songs based on their features and compared its predictions to my own labels. The results of this test are shown in the table on the left.

Overall, the model performed well, correctly identifying 15 out of the 20 songs in the playlist as either songs I would like or dislike. This provides further evidence that the model is a useful tool for understanding my music preferences and could be applied to other playlists or datasets.

### Viewing the metrics of the model performance on the New Music Friday (NL) Spotify playlist

```{r display the stats the decision tree fitted to new music friday}
# Compute the confusion matrix
nmf_conf_mat <- conf_mat(nmf_pred, truth = truth, estimate = estimate)

# Plot confusion matrix as heatmap
heatmap2 <- nmf_conf_mat |>
            autoplot(type = "heatmap", return_ggplot = TRUE) + 
            scale_fill_gradientn(colors = brewer.pal(9, "Blues")) +
            ggtitle("Confusion Matrix of the model predicting labels\non the 'New Music Friday' playlist")

heatmap2
```

---

The results of the previous model performance evaluation can be observed in the data presented here. The analysis indicates that the model was slightly more successful in predicting whether I would like a song, as opposed to predicting whether I would dislike a song. Despite this, the model performed well overall and was able to accurately classify a majority of the songs in the New Music Friday (NL) Spotify playlist.

```{r stats2}
# Get precision and recall
pr2 <- nmf_conf_mat[["table"]] |> get_pr2()

# Display table of precision and recall
knitr::kable(pr2, caption = "Precision and Recall of the Decision Tree classifying the 'New Music Friday (NL)' playlist from Spotify")
```

Conclusion {data-icon="ion-document"}
=====================================

Column 1
--------------------------------------------------

###

**Conclusion:**

After a comprehensive analysis of my music preferences using various tools and techniques, I have gained valuable insights into the factors that influence my musical taste. Through the analysis of chromagrams, ceptrograms, tempograms, and self-similarity matrices, I have been able to observe the harmonic and timbral characteristics of different songs, as well as the impact of tempo and song structure on my preferences.

The results have revealed key features that differentiate between the "Songs I Like" and "Songs I Dislike" playlists, with low instrumentalness, low speechiness, and energy levels below 0.89 being the most important features in songs that I like. Moreover, the analysis of self-similarity matrices showed that I tend to prefer songs with a clear and familiar structure, while also appreciating songs with unconventional arrangements.

The decision tree classification model was successfully trained to predict personal musical preferences based on these identified features. The model was evaluated using the original dataset and was able to accurately classify a majority of the songs into either the "Songs I Like" or "Songs I Dislike" playlists. Furthermore, the model was tested on an unknown playlist ("New Music Friday") and was able to predict which songs would align with my personal taste with a high level of accuracy.

The results of this analysis have implications for music streaming platforms such as Spotify, as they provide insights into users' preferences and allow for more personalized music recommendations. Additionally, music producers and composers could use these conclusions to better understand the elements that make up popular songs and to tailor their compositions to appeal to specific audiences.

Overall, this study has provided me with a better understanding of my music preferences, and has highlighted the importance of various musical features in determining my preferences. The success of the decision tree classification model in predicting my personal musical preferences also demonstrates its potential utility for predicting the preferences of other listeners.

Column 1
--------------------------------------------------

###

**References:**

Baker, D. J. (@davidjohnbaker1) & Burgoyne, J. A. (@jaburgoyne). (2023). compmus2023. GitHub repository. https://github.com/jaburgoyne/compmus2023

Müller, M. (2015). Fundamentals of music processing: Audio, analysis, algorithms, applications. Springer.

Serrà, J., Corral, Á., Boguñá, M., Haro, M., & Ll Arcos, J. (2012). Measuring the evolution of contemporary western popular music. Scientific Reports, 2, 521. https://doi.org/10.1038/srep00521

spotifyr package. (n.d.). Spotify Developer. Retrieved April 2, 2023, from https://developer.spotify.com/
