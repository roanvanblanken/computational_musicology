---
title: "Computational Musicology - Final Portfolio"
author: "Roan van Blanken"
output: 
  flexdashboard::flex_dashboard:
    orientation: columns
    self_contained: false
---

```{r setup, include=FALSE}
library(flexdashboard)
library(spotifyr)
library(purrr)
library(dplyr)
library(ggplot2)
library(plotly)
library(tidyverse)
library(compmus)
library(fontawesome)

# Get playlists
songs_i_like <- get_playlist_audio_features("", "6pl0C7qbIl5uoY3Tdf82oa")

songs_i_like <- songs_i_like %>%
  mutate(mode = ifelse(mode == 0, "Minor", "Major"))

songs_i_dislike <- get_playlist_audio_features("", "4bJQX5w7W4wEnHLmWqUIVY")

songs_i_dislike <- songs_i_dislike %>%
  mutate(mode = ifelse(mode == 0, "Minor", "Major"))

# Define a custom color palette
random_palette <- colorRampPalette(c("#2c7bb6", "#abdda4", "#ffffbf", "#fdae61", "#d7191c"))(100)
blue_red <- colorRampPalette(c("#00008B", "#ADD8E6", "#E6FFFF", "#FFE4E1", "#FF6347", "#8B0000"))(100)
```

```{r chords and keys setup, include=FALSE}
circshift <- function(v, n) {
  if (n == 0) v else c(tail(v, n), head(v, -n))
}

#      C     C#    D     Eb    E     F     F#    G     Ab    A     Bb    B
major_chord <-
  c(   1,    0,    0,    0,    1,    0,    0,    1,    0,    0,    0,    0)
minor_chord <-
  c(   1,    0,    0,    1,    0,    0,    0,    1,    0,    0,    0,    0)
seventh_chord <-
  c(   1,    0,    0,    0,    1,    0,    0,    1,    0,    0,    1,    0)

major_key <-
  c(6.35, 2.23, 3.48, 2.33, 4.38, 4.09, 2.52, 5.19, 2.39, 3.66, 2.29, 2.88)
minor_key <-
  c(6.33, 2.68, 3.52, 5.38, 2.60, 3.53, 2.54, 4.75, 3.98, 2.69, 3.34, 3.17)

chord_templates <-
  tribble(
    ~name, ~template,
    "Gb:7", circshift(seventh_chord, 6),
    "Gb:maj", circshift(major_chord, 6),
    "Bb:min", circshift(minor_chord, 10),
    "Db:maj", circshift(major_chord, 1),
    "F:min", circshift(minor_chord, 5),
    "Ab:7", circshift(seventh_chord, 8),
    "Ab:maj", circshift(major_chord, 8),
    "C:min", circshift(minor_chord, 0),
    "Eb:7", circshift(seventh_chord, 3),
    "Eb:maj", circshift(major_chord, 3),
    "G:min", circshift(minor_chord, 7),
    "Bb:7", circshift(seventh_chord, 10),
    "Bb:maj", circshift(major_chord, 10),
    "D:min", circshift(minor_chord, 2),
    "F:7", circshift(seventh_chord, 5),
    "F:maj", circshift(major_chord, 5),
    "A:min", circshift(minor_chord, 9),
    "C:7", circshift(seventh_chord, 0),
    "C:maj", circshift(major_chord, 0),
    "E:min", circshift(minor_chord, 4),
    "G:7", circshift(seventh_chord, 7),
    "G:maj", circshift(major_chord, 7),
    "B:min", circshift(minor_chord, 11),
    "D:7", circshift(seventh_chord, 2),
    "D:maj", circshift(major_chord, 2),
    "F#:min", circshift(minor_chord, 6),
    "A:7", circshift(seventh_chord, 9),
    "A:maj", circshift(major_chord, 9),
    "C#:min", circshift(minor_chord, 1),
    "E:7", circshift(seventh_chord, 4),
    "E:maj", circshift(major_chord, 4),
    "G#:min", circshift(minor_chord, 8),
    "B:7", circshift(seventh_chord, 11),
    "B:maj", circshift(major_chord, 11),
    "D#:min", circshift(minor_chord, 3)
  )

key_templates <-
  tribble(
    ~name, ~template,
    "Gb:maj", circshift(major_key, 6),
    "Bb:min", circshift(minor_key, 10),
    "Db:maj", circshift(major_key, 1),
    "F:min", circshift(minor_key, 5),
    "Ab:maj", circshift(major_key, 8),
    "C:min", circshift(minor_key, 0),
    "Eb:maj", circshift(major_key, 3),
    "G:min", circshift(minor_key, 7),
    "Bb:maj", circshift(major_key, 10),
    "D:min", circshift(minor_key, 2),
    "F:maj", circshift(major_key, 5),
    "A:min", circshift(minor_key, 9),
    "C:maj", circshift(major_key, 0),
    "E:min", circshift(minor_key, 4),
    "G:maj", circshift(major_key, 7),
    "B:min", circshift(minor_key, 11),
    "D:maj", circshift(major_key, 2),
    "F#:min", circshift(minor_key, 6),
    "A:maj", circshift(major_key, 9),
    "C#:min", circshift(minor_key, 1),
    "E:maj", circshift(major_key, 4),
    "G#:min", circshift(minor_key, 8),
    "B:maj", circshift(major_key, 11),
    "D#:min", circshift(minor_key, 3)
  )

```

Introduction {data-icon="fa-spotify"}
=====================================

Column {data-width=500}
-----------------------------------------------------------------------

### Introduction

Music is an essential part of many people's lives, and with digital music streaming platforms like Spotify, discovering new music has become more accessible than ever. Spotify's algorithm suggests playlists based on users' listening habits and preferences. One of the most popular playlists on Spotify is "New Music Friday," updated every week with new releases from various artists. It is curated by Spotify's editorial team, who select the latest and most popular releases from various genres. However, the question remains: how accurately does this playlist align with personal musical taste and style?

This final portfolio explores the inner workings of Spotify's music recommendation algorithm, attempting to create a predictive model that accurately predicts a user's preferred musical genres and styles. The analysis will focus on two playlists - "Songs I Like" and "Songs I Dislike" - to identify patterns in personal musical taste. The "Songs I Like" playlist is a collection of songs that hold personal meaning, representing various genres such as pop, 70s, 80s, 90s musical, and rock. In contrast, the "Songs I Dislike" playlist consists of heavy metal and drill rap songs that do not match personal taste.

By analyzing the features of songs in both playlists, this portfolio aims to provide insights into personal musical preferences and use this information to train a machine learning model. The goal is to create a predictive model that accurately predicts preferred musical genres and styles. Additionally, the analysis will investigate whether this model can predict which songs from popular playlists like "New Music Friday" would resonate with the user, allowing for a more personalized and tailored listening experience.

In summary, this portfolio tries to offer an understanding of Spotify's music recommendation algorithm and the potential for using machine learning to predict a user's preferred musical genres and styles.

Column {data-width=250}
-----------------------------------------------------------------------

### Corpus: Songs I like

iframe src="https://open.spotify.com/embed/playlist/6pl0C7qbIl5uoY3Tdf82oa?utm_source=generator" width="100%" height="100%" frameBorder="0" allowfullscreen="" allow="autoplay; clipboard-write; encrypted-media; fullscreen; picture-in-picture"></iframe>


Column {data-width=250}
-----------------------------------------------------------------------

### Corpus: Songs I dislike

iframe src="https://open.spotify.com/embed/playlist/4bJQX5w7W4wEnHLmWqUIVY?utm_source=generator&theme=0" width="100%" height="100%" frameBorder="0" allowfullscreen="" allow="autoplay; clipboard-write; encrypted-media; fullscreen; picture-in-picture"></iframe>

Feature analysis {.storyboard data-icon="fa-magnifying-glass"}
=====================================

### Relationship between Valence, Energy, Loudness, and Mode for the songs I like 

```{r songs_i_like plot}
# Create a scatterplot
songs_i_like %>%
  ggplot(aes(x = valence, y = energy, size = loudness, color = mode)) +
  geom_point(alpha = 0.5) +
  scale_x_continuous(limits = c(0, 1), breaks = c(0, 0.50, 1), minor_breaks = NULL) +
  scale_y_continuous(limits = c(0, 1), breaks = c(0, 0.50, 1), minor_breaks = NULL) +
  scale_color_manual(values = c("darkblue", "lightblue"), name = "Mode", labels = c("Minor", "Major")) +
  scale_size_continuous(trans = "exp", range = c(1, 10), guide = guide_legend(override.aes = list(size = c(1, 3, 5)))) +
  theme_light() +
  labs(x = "Valence", y = "Energy",
       title = "Songs I like",
       subtitle = "Plotting valence, energy, and loudness",
       caption = "Data source: Spotify API\nAuthor: Roan van Blanken")

```

---

Spotify's track-level features, such as valence, danceability, energy, and loudness, offer valuable insights into the underlying patterns of my personal music preferences. I selected these features as they are commonly used to describe the overall mood, intensity, and emotional tone of a song, allowing us to better understand the characteristics that define the songs I like and dislike [(Serra et al.)](https://pubmed.ncbi.nlm.nih.gov/22837813/). By examining these features, we can gain a deeper understanding of the traits that influence my musical taste.

Based on the first scatterplot, it seems like I tend to like songs that are loud, with a range of loudness values represented in the plot. I also seem to be drawn to songs that are high-energy, as they're clustered towards the upper-right portion of the plot where both the energy and loudness values are high.

As for the valence of the songs I like, it appears to be centered around 0.5, but mostly falls between 0.5 and 1.0. This suggests that I tend to prefer songs with a positive emotional valence, which could contribute to their appeal.

One interesting thing I noticed in the plot is that the songs I like are equally distributed between major and minor keys, as indicated by the color coding in the plot. This suggests that the mode of the songs I like doesn't strongly influence my preference for them.

### Relationship between Valence, Energy, Loudness, and Mode for the songs I dislike

```{r songs_i_dislike plot}
# Create a scatterplot
songs_i_dislike %>%
  ggplot(aes(x = valence, y = energy, size = loudness, color = mode)) +
  geom_point(alpha = 0.5) +
  scale_x_continuous(limits = c(0, 1), breaks = c(0, 0.50, 1), minor_breaks = NULL) +
  scale_y_continuous(limits = c(0, 1), breaks = c(0, 0.50, 1), minor_breaks = NULL) +
  scale_color_manual(values = c("darkred", "lightcoral")) +
  scale_size_continuous(trans = "exp", range = c(1, 10), guide = guide_legend(override.aes = list(size = c(1, 3, 5, 7)))) +
  theme_light() +
  labs(x = "Valence", y = "Energy",
       title = "Songs I dislike",
       subtitle = "Plotting valence, energy, loudness, and mode",
       caption = "Data source: Spotify API\nAuthor: Roan van Blanken")
```

---

Looking at the scatterplot of songs I don't like, I can see that they're usually not as loud as the songs I enjoy. Even though they still have high energy levels, they often have lower valence. This makes me believe that I might like songs that have a more positive and uplifting vibe, rather than ones that are more downbeat or sad.

It's interesting to see that, just like the songs I like, the songs I don't like are spread out pretty evenly between major and minor keys. This tells me that the key of a song doesn't really play a big role in whether I like it or not.

Another thing I noticed is that the songs I don't like seem to have a smaller range of loudness and energy compared to the songs I do like. This could mean that I'm more open to different levels of intensity and dynamics when it comes to music I enjoy, while the music I don't like might have more in common in terms of how loud and energetic they are.

### Zooming in on Valence

```{r valence_plot}
plot_ly() %>%
  add_trace(data = songs_i_like, x = ~valence, type = "histogram", 
            histnorm = "probabilty density", name = "Songs I like", 
            marker = list(color = "lightblue", opacity = 0.5)) %>%
  add_trace(data = songs_i_dislike, x = ~valence, type = "histogram", 
            histnorm = "probabilty density", name = "Songs I dislike", 
            marker = list(color = "lightcoral", opacity = 0.5)) %>%
  layout(xaxis = list(title = "Valence"), yaxis = list(title = "Density"),
         title = "Valence Distribution",
         showlegend = TRUE,
         legend = list(title = "", orientation = "h"))
```

---

This plot provides a visual representation of how valence is distributed among two different sets of songs: 'Songs I like' and 'Songs I dislike'.

Valence is a measure of how positive or negative a musical piece sounds, with a scale that ranges from negative to positive values. The plot shows two histograms, one in light blue representing the valence distribution for the songs that I like, and the other in light coral representing the valence distribution for the songs that I dislike.

By looking at the plot, we can see that the valence distribution for the songs that I like is skewed towards the positive end of the spectrum, indicating that the songs the I like tend to have a more positive sound. In contrast, the valence distribution for the songs that I dislike is more skewed towards the negative end of the spectrum, suggesting that the songs I dislike have a more negative sound.

### Zooming in on Energy

```{r energy_plot}
plot_ly() %>%
  add_trace(data = songs_i_like, x = ~energy, type = "histogram", 
            histnorm = "probabilty density", name = "Songs I like", 
            marker = list(color = "lightgreen", opacity = 0.5)) %>%
  add_trace(data = songs_i_dislike, x = ~energy, type = "histogram", 
            histnorm = "probabilty density", name = "Songs I dislike", 
            marker = list(color = "coral", opacity = 0.5)) %>%
  layout(xaxis = list(title = "Energy"), yaxis = list(title = "Density"),
         title = "Energy Distribution",
         showlegend = TRUE,
         legend = list(title = "", orientation = "h"))
```

---

This plot provides insight into the energy distribution of two different sets of songs: 'Songs I like' and 'Songs I dislike'. Energy is a measure of how intense and active a musical piece is, with a scale that ranges from low to high values. The plot displays two histograms, one in light green representing the energy distribution for the songs that I like, and the other in coral representing the energy distribution for the songs that I dislike.

Upon examining the plot, we can see that the energy distribution for the songs that I like is concentrated between 0.7 and 0.9, with a peak around 0.8. This indicates that the songs that I enjoy tend to have a moderate level of energy, without being too intense or too mellow. In contrast, the energy distribution for the songs that I dislike is concentrated around 0.95, indicating that the songs that I find unfavorable tend to be more energetic.

The findings of this plot suggest that energy may be an important factor in shaping my musical preferences. However, it is essential to note that energy is just one of many factors that can influence an individual's musical tastes.

### Zooming in on Loudness

```{r loudness_plot}
plot_ly() %>%
  add_trace(data = songs_i_like, x = ~loudness, type = "histogram", 
            histnorm = "probabilty density", name = "Songs I like", 
            marker = list(color = "#00BFC4", opacity = 0.5)) %>%
  add_trace(data = songs_i_dislike, x = ~loudness, type = "histogram", 
            histnorm = "probabilty density", name = "Songs I dislike", 
            marker = list(color = "#F8766D", opacity = 0.5))%>%
  layout(xaxis = list(title = "Loudness"), yaxis = list(title = "Density"),
         title = "Loudness Distribution",
         showlegend = TRUE,
         legend = list(title = "", orientation = "h"))
```

---

Musical preferences are complex and can be influenced by various factors, including the loudness of a musical piece. Loudness is an essential characteristic of music that can affect how it is perceived by the listener. It refers to the volume or intensity of a musical piece and is typically measured in decibels.

The plot that highlights the difference in loudness between songs that I like and songs that I dislike may provide insight into how loudness influences my musical preferences. The plot may indicate that my preferred songs tend to be louder than songs that I dislike, suggesting that loudness is an essential characteristic of my musical taste. However, it is crucial to remember that loudness is just one factor that can influence musical preferences.

It is worth noting that loudness is not always a reliable indicator of a songs quality or emotional impact. A quiet ballad or a soothing instrumental piece can be just as powerful and moving as a loud and energetic track. Therefore, while loudness may be an important characteristic of my preferred songs, it should not be the only factor considered when evaluating the quality or emotional impact of a musical piece.

Track-Level Summary {data-icon="fa-chart-bar"}
=====================================

Overview
--------------------------------------------------

### Outliers

In any research study, selecting an appropriate corpus is crucial in effectively achieving the research objectives. In the present study, a broad corpus was chosen, which included diverse data related to the research question. However, the large volume of data made it challenging to identify significant patterns or trends that could adequately address the research question.

To overcome this challenge, the decision was made to focus on the outliers in the corpus. Specifically, the analysis focused on the extreme cases that were most divergent from the norm in terms of a specific timbre component. This approach allowed for the isolation and study of the outliers, leading to valuable insights and a better understanding of the factors contributing to their unique timbre characteristics. Ultimately, this approach strengthened the analysis and enhanced the quality of the research findings.

In this study, timbre, the quality of sound that distinguishes different musical instruments, was analyzed using spectral content, which measures the relative strengths of various frequency components that make up the sound. The study focused on a specific timbre component, which was used to isolate and study the outliers in the corpus.

To further investigate this approach, tables were generated to identify the maximum and minimum values of specific timbre components for each playlist. The tables for the maximum and minimum values of c01, c02, c03, and c04 for both the 'Songs I like' and 'Songs I dislike' playlists showed notable differences in timbre components between the two playlists. For example, the table of maximum and minimum values of c02 for each playlist showed that the highest and lowest timbre values for 'Songs I like' were exhibited by the songs ['What I Like About You'](https://open.spotify.com/track/4ebcE2SmkG7nplvzFAWRu7?si=c432fccad51845ae), and ['The Sailorâ€™s Warning'](https://open.spotify.com/track/3bgmoyCHa6v3bMMsgK1rWh?si=d7b40ddaef5344a1), respectively, while the highest and lowest timbre values for 'Songs I dislike' were exhibited by the songs ['Murder'](https://open.spotify.com/track/0wFPKFtBes3NTyTW4bkjrz?si=796b185ece5d4a75) and ['19 Tini 5'](https://open.spotify.com/track/73myy2XXpvC2bHi4Fikksr?si=775791f95d43461a), respectively.

These tables provide insight into how specific timbre components differ between songs that I like and songs that I dislike, which helps to support the approach of focusing on the outliers in the corpus to gain valuable insights into timbre characteristics.

Column 2 {.tabset}
--------------------------------------------------

### Timbre components

```{r timbre_summary}
timbre_data <- readRDS(file="data/timbre_data.Rda")

plot1 <- timbre_data %>%
  ggplot(aes(x = factor(basis), y = value, fill = Playlist)) +
  geom_violin() +
  scale_fill_viridis_d() +
  labs(x = "Spotify Timbre Coefficients", y = "", fill = "Playlist") +
  theme_minimal()

ggplotly(plot1)
```

### c01

```{r c01}
# Create table for c01
table_c01 <- timbre_data %>%
  filter(basis == "c01") %>%
  group_by(Playlist) %>%
  slice_min(value, n = 1) %>%
  bind_rows(timbre_data %>%
              filter(basis == "c01") %>%
              group_by(Playlist) %>%
              slice_max(value, n = 1)) %>%
  select(Playlist, track.name, artists, value) %>%
  rename("Track" = "track.name", "Artists" = "artists")

knitr::kable(table_c01, caption = "Table of maximum and minimum values of c01 for each playlist")

```

### c02

```{r c02}
# Create table for c02
table_c02 <- timbre_data %>%
  filter(basis == "c02") %>%
  group_by(Playlist) %>%
  slice_min(value, n = 1) %>%
  bind_rows(timbre_data %>%
              filter(basis == "c02") %>%
              group_by(Playlist) %>%
              slice_max(value, n = 1)) %>%
  select(Playlist, track.name, artists, value) %>%
  rename("Track" = "track.name", "Artists" = "artists")

knitr::kable(table_c02, caption = "Table of maximum and minimum values of c02 for each playlist")
```

### c03

```{r c03}
# Create table for c03
table_c03 <- timbre_data %>%
  filter(basis == "c03") %>%
  group_by(Playlist) %>%
  slice_min(value, n = 1) %>%
  bind_rows(timbre_data %>%
              filter(basis == "c03") %>%
              group_by(Playlist) %>%
              slice_max(value, n = 1)) %>%
  select(Playlist, track.name, artists, value) %>%
  rename("Track" = "track.name", "Artists" = "artists")

knitr::kable(table_c03, caption = "Table of maximum and minimum values of c03 for each playlist")
```

### c04

```{r c04}
# Create table for c04
table_c04 <- timbre_data %>%
  filter(basis == "c04") %>%
  group_by(Playlist) %>%
  slice_min(value, n = 1) %>%
  bind_rows(timbre_data %>%
              filter(basis == "c04") %>%
              group_by(Playlist) %>%
              slice_max(value, n = 1)) %>%
  select(Playlist, track.name, artists, value) %>%
  rename("Track" = "track.name", "Artists" = "artists")

knitr::kable(table_c04, caption = "Table of maximum and minimum values of c04 for each playlist")
```


Chromagrams {data-icon="fa-music"}
=====================================

Text about chromagrams {data-width=400}
--------------------------------------------------

###

Chromagrams are powerful tools for analyzing the harmonic content and structural features of music. In this portfolio, we examined two songs from my musical corpus: "What I Like About You" from the playlist 'Songs I Like' and "Murder" from the playlist 'Songs I Dislike'. These songs were analyzed using their respective chromagrams to gain insight into their harmonic structures and to better understand my personal musical preferences.

["What I Like About You"](https://open.spotify.com/track/4ebcE2SmkG7nplvzFAWRu7?si=c432fccad51845ae) by The Romantics has a simple harmonic structure that consists of the chords A, G#, E, and D. The corresponding chromagram for this song shows a clear and consistent distribution of these pitches. This simple harmonic structure may contribute to why I enjoy this song and have included it in my 'Songs I Like' playlist.

On the other hand, ["Murder"](https://open.spotify.com/track/0wFPKFtBes3NTyTW4bkjrz?si=796b185ece5d4a75) has a more complex harmonic structure that includes frequent modulations but tends to avoid the pitches C# and C. The chromagram for this song displays a diverse distribution of pitches, indicating a more intricate harmonic structure and chord progression. This variation in pitch distribution may be the reason why I dislike this song and have included it in my 'Songs I Dislike' playlist.

By comparing the chromagrams of these two songs, we can gain insights into the underlying harmonic structures that contribute to my personal musical taste. The analysis of chromagrams allows us to identify commonalities and differences between songs, which can be used to curate playlists that align with our individual musical preferences.


Chromagrams {data-width=600}
--------------------------------------------------

###

```{r chromagram1}
wilay <-
  get_tidy_audio_analysis("4ebcE2SmkG7nplvzFAWRu7") |>
  select(segments) |>
  unnest(segments) |>
  select(start, duration, pitches)

chromogram1 <- wilay |>
  mutate(pitches = map(pitches, compmus_normalise, "euclidean")) |>
  compmus_gather_chroma() |> 
  ggplot(
    aes(
      x = start + duration / 2,
      width = duration,
      y = pitch_class,
      fill = value
    )
  ) +
  geom_tile() +
  ggtitle("Chromagram - The Romantics: 'What I Like About You'") +
  scale_fill_gradientn(colors = random_palette) +
  labs(x = "Time (s)", y = NULL, fill = "Magnitude") +
  theme_minimal() +
  scale_color_gradientn(colors = random_palette)

ggplotly(chromogram1, tooltip = "none", source = "none")
```

###

```{r chromagram 2}
murder <-
  get_tidy_audio_analysis("0wFPKFtBes3NTyTW4bkjrz") |>
  select(segments) |>
  unnest(segments) |>
  select(start, duration, pitches)

chromogram2 <- murder |>
  mutate(pitches = map(pitches, compmus_normalise, "euclidean")) |>
  compmus_gather_chroma() |> 
  ggplot(
    aes(
      x = start + duration / 2,
      width = duration,
      y = pitch_class,
      fill = value
    )
  ) +
  geom_tile() +
  ggtitle("Chromagram - DJ Squeeky, Tom Skeemask, GK: 'Murder'") +
  scale_fill_gradientn(colors = random_palette) +
  labs(x = "Time (s)", y = NULL, fill = "Magnitude") +
  theme_minimal() +
  scale_color_gradientn(colors = random_palette)

ggplotly(chromogram2, tooltip = "none", source = "none")
```

Ceptrograms {data-icon="fa-headphones-alt"}
=====================================

Text about ceptrograms {data-width=400}
--------------------------------------------------

###

Ceptrograms are an essential tool for analyzing the timbral characteristics of a musical recording. A ceptrogram provides a visual representation of the distribution of timbral characteristics such as brightness, warmth, and depth in a song. By comparing the ceptrograms of two songs, we can gain insights into the similarities and differences in their timbral characteristics and use this knowledge to better understand our personal musical preferences.

In this portfolio, we analyzed two songs from my corpus, ["Shut Up and Dance"](https://open.spotify.com/track/4kbj5MwxO1bq9wjT5g9HaA?si=caf4a51f4d4645c4) from the 'Songs I Like' playlist and ["State of Unrest"](https://open.spotify.com/track/3u4djE2yAEkKMWJEUOOJyT?si=1230a654e318462a) from the 'Songs I Dislike' playlist, using their respective ceptrograms to understand their timbral structures and to gain insights into why I might have included them in their respective playlists.

Upon analyzing the ceptrogram of "Shut Up and Dance," we observed a strong presence around the c01 and c02 timbre coefficients. These coefficients indicate that the song has a bright and loud timbral quality, which may contribute to why I enjoy listening to it and why it is included in my 'Songs I Like' playlist.

On the other hand, "State of Unrest" exhibits a similar distribution of timbral characteristics as "Shut Up and Dance," with a strong presence around the c01 and c02 timbre coefficients. However, this song is included in my 'Songs I Dislike' playlist, indicating that timbre is not the main factor that determines my musical preferences.

In conclusion, the analysis of ceptrograms for the songs "Shut Up and Dance" and "State of Unrest" provides us with insights into their timbral structures and how they may contribute to our personal musical preferences. While "Shut Up and Dance" exhibits a bright and loud timbral quality that may explain why it is included in the 'Songs I Like' playlist, "State of Unrest" has a similar distribution of timbral characteristics but is included in the 'Songs I Dislike' playlist, indicating that other factors beyond timbre play a role in determining our musical preferences.


Ceptrograms {data-width=600}
--------------------------------------------------

###

```{r ceptrogram 1}
state_of_unrest <- readRDS(file="data/Ceptrogram (state_of_unrest).Rda")

ceptrogram1 <- state_of_unrest |>
  ggplot(aes(x = start + duration / 2, width = duration, y = basis, fill = value)) +
  geom_tile() +
  ggtitle("Ceptrogram - Lamb of God, Kreator: 'State of Unrest'") +
  scale_fill_gradientn(colors = random_palette) +
  labs(x = "Time (s)", y = NULL, fill = "Magnitude") +
  theme_minimal() +
  scale_color_gradientn(colors = random_palette)

ggplotly(ceptrogram1, tooltip = "none", source = "none")
```

###

```{r ceptrogram 2}
shutupanddance <- readRDS(file="data/Ceptrogram (shutupanddance).Rda")

ceptrogram2 <- shutupanddance |>
  ggplot(aes(x = start + duration / 2, width = duration, y = basis, fill = value)) +
  geom_tile() +
  ggtitle("Ceptrogram - WALK THE MOON: 'Shut up and Dance'") +
  scale_fill_gradientn(colors = random_palette) +
  labs(x = "Time (s)", y = NULL, fill = "Magnitude") +
  theme_minimal() +
  scale_color_gradientn(colors = random_palette)

ggplotly(ceptrogram2, tooltip = "none", source = "none")
```


Tempograms {data-icon="fas fa-drum"}
=====================================

Text about tempograms {data-width=400}
--------------------------------------------------

###

Tempo, which refers to the speed or pace of a piece of music, is an essential aspect that greatly influences listeners' perception and emotional response to music. Some songs have a slow and steady tempo that can evoke feelings of calmness and relaxation, while others have a fast and upbeat tempo that may evoke feelings of excitement and energy. Tempograms are visual representations of the tempo fluctuations in a piece of music over time. They provide a useful tool for analyzing and comparing the tempo patterns of different songs, allowing us to gain insights into the structure and dynamics of musical compositions.

To explore the range of tempos in the 'Songs I like' playlist, I have selected two songs with the highest and lowest tempos, namely ['High Hopes'](https://open.spotify.com/track/1rqqCSm0Qe4I9rUvWncaom?si=14d92d54789c4081) by Panic! At The Disco with a tempo of 82.014 and ['Fascination'](https://open.spotify.com/track/5AKQ1JHezaXDmN5SyMSpEr?si=6478428cf0cf45a7) by Alphabeat with a tempo of 193.968. These songs offer a glimpse into the variety of tempos found in the playlist and provide an opportunity to observe any tempo changes in slow and fast songs.

Upon analyzing the Tempograms, it can be observed that both songs maintain a relatively consistent tempo with occasional variations. 'High Hopes' mostly maintains a tempo of around 80 BMP, while 'Fascination' hovers around 98 BPM. However, 'High Hopes' displays more frequent tempo changes compared to 'Fascination.'

In conclusion, the selected songs from the 'Songs I like' playlist offer insights into the range of tempos in music that I enjoy. The Tempogram analysis has also revealed interesting patterns in tempo fluctuations, which can further enhance our understanding of the role of tempo in music taste.


Tempograms {data-width=600}
--------------------------------------------------

### 

```{r tempogram_1}
high_hopes_cyclic <- readRDS(file="data/Tempogram (high_hopes_cyclic).Rda")

tempogram_1 <- high_hopes_cyclic |>
  ggplot(aes(x = time, y = bpm, fill = power)) +
  geom_raster() +
  ggtitle("Cyclic Tempogram - Panic! At The Disco: 'High Hopes'") +
  scale_fill_gradientn(colors = random_palette) +
  labs(x = "Time (s)", y = "Tempo (BPM)") +
  theme_minimal() +
  scale_color_gradientn(colors = random_palette)

ggplotly(tempogram_1, tooltip = "none", source = "none")
```

### 

```{r tempogram_2}
fascination_cyclic <- readRDS(file="data/Tempogram (fascination_cyclic).Rda")

tempogram_2 <- fascination_cyclic |>
  ggplot(aes(x = time, y = bpm, fill = power)) +
  geom_raster() +
  ggtitle("Cyclic Tempogram - Alphabeat: 'Fascination'") +
  scale_fill_gradientn(colors = random_palette) +
  labs(x = "Time (s)", y = "Tempo (BPM)") +
  theme_minimal() +
  scale_color_gradientn(colors = random_palette)

ggplotly(tempogram_2, tooltip = "none", source = "none")
```

Self-similarity Matrices {data-icon="fas fa-share-from-square"}
=====================================

text over SSM {data-width=500}
--------------------------------------------------

###

A self-similarity matrix is a visual representation of the similarity between different sections of a musical recording. This tool allows us to observe and analyze the internal structure of a song in a way that provides valuable insights into its musical composition and arrangement. By comparing the self-similarity matrices of two songs, one I like a lot and one I dislike, ["State of Unrest"](https://open.spotify.com/track/3u4djE2yAEkKMWJEUOOJyT?si=1230a654e318462a) and ["Shut Up and Dance"](https://open.spotify.com/track/4kbj5MwxO1bq9wjT5g9HaA?si=caf4a51f4d4645c4) in both timbre and chroma, we can gain a deeper understanding of how these songs are structured and how they differ from each other.

When analyzing the self-similarity matrix for the timbral characteristics of the songs, we can see variations in the repetition of certain musical motifs. These motifs could be rhythmic patterns, melodies, or harmonies that repeat throughout the song, providing a sense of continuity and coherence. In the case of "State of Unrest" and "Shut Up and Dance," we can see that the two songs differ in terms of the frequency and regularity of these motifs. "Shut Up and Dance" has a more consistent and structured pattern of repetition, while "State of Unrest" has a less predictable and more sporadic one.

Similarly, the chroma self-similarity matrix provides insights into how the songs' chord progressions and tonal centers change over time. This information is particularly valuable in understanding the harmonic structure of a song, which is essential in determining its overall mood and emotional impact. In the case of "State of Unrest" and "Shut Up and Dance," we can see that the two songs differ in terms of their harmonic complexity and diversity. "Shut Up and Dance" has a more straightforward and predictable harmonic structure, while "State of Unrest" explores a wider range of harmonic possibilities.

One notable difference between the two songs is that, around the 150-second mark, the bridge section in "Shut Up and Dance" is clearly visible in both timbre and chroma self-similarity matrices, while in "State of Unrest," this section is less structured and organized. 

Through this analysis, I can see that "Shut Up and Dance" follows a more traditional song structure, with a clear and predictable form that includes defined verses, choruses, and bridges. In contrast, "State of Unrest" may have a more unconventional arrangement, with a less predictable and more sporadic pattern of repetition. While both songs have their unique qualities, I tend to respond positively to songs that have a clear and familiar structure, and this could be one of the reasons why I find "Shut Up and Dance" more appealing than "State of Unrest."

SSM {data-width=250}
--------------------------------------------------

###

```{r compmus_self_similarity1}
state_of_unrest_chroma <- readRDS("data/Self-similarity Matrix (state_of_unrest_chroma).Rda")

ssm1 <- state_of_unrest_chroma |> 
  ggplot(
    aes(
      x = xstart + xduration / 2,
      width = xduration,
      y = ystart + yduration / 2,
      height = yduration,
      fill = d
    )
  ) +
  geom_tile() +
  ggtitle("Self-similarity Matrix (Chroma)\nLamb of God, Kreator: 'State of Unrest'") +
  scale_fill_gradientn(colors = blue_red) +
  labs(x = NULL, y = NULL, fill = "") +
  theme_minimal() +
  scale_color_gradientn(colors = blue_red) +
  theme(plot.title = element_text(size = 10))

ggplotly(ssm1, tooltip = "none", source = "none")
```


###

```{r compmus_self_similarity2}
state_of_unrest_timbre <- readRDS("data/Self-similarity Matrix (state_of_unrest_timbre).Rda")

ssm2 <- state_of_unrest_timbre |> 
  ggplot(
    aes(
      x = xstart + xduration / 2,
      width = xduration,
      y = ystart + yduration / 2,
      height = yduration,
      fill = d
    )
  ) +
  geom_tile() +
  ggtitle("Self-similarity Matrix (Timbre)\nLamb of God, Kreator: 'State of Unrest'") +
  scale_fill_gradientn(colors = blue_red) +
  labs(x = NULL, y = NULL, fill = "") +
  theme_minimal() +
  scale_color_gradientn(colors = blue_red) +
  theme(plot.title = element_text(size = 10))

ggplotly(ssm2, tooltip = "none", source = "none")
```

column3 {data-width=250}
--------------------------------------------------

###

```{r compmus_self_similarity3}
shutupanddance_chroma <- readRDS("data/Self-similarity Matrix (shutupanddance_chroma).Rda")

ssm3 <- shutupanddance_chroma |> 
  ggplot(
    aes(
      x = xstart + xduration / 2,
      width = xduration,
      y = ystart + yduration / 2,
      height = yduration,
      fill = d
    )
  ) +
  geom_tile() +
  ggtitle("Self-similarity Matrix (Chroma)\nWALK THE MOON: 'Shut up and Dance'") +
  scale_fill_gradientn(colors = blue_red) +
  labs(x = NULL, y = NULL, fill = "") +
  theme_minimal() +
  scale_color_gradientn(colors = blue_red) +
  theme(plot.title = element_text(size = 10))

ggplotly(ssm3, tooltip = "none", source = "none")
```

###

```{r compmus_self_similarity4}
shutupanddance_timbre <- readRDS("data/Self-similarity Matrix (shutupanddance_timbre).Rda")

ssm4 <- shutupanddance_timbre |> 
  ggplot(
    aes(
      x = xstart + xduration / 2,
      width = xduration,
      y = ystart + yduration / 2,
      height = yduration,
      fill = d
    )
  ) +
  geom_tile() +
  ggtitle("Self-similarity Matrix (Timbre)\nWALK THE MOON: 'Shut up and Dance'") +
  scale_fill_gradientn(colors = blue_red) +
  labs(x = NULL, y = NULL, fill = "") +
  theme_minimal() +
  scale_color_gradientn(colors = blue_red) +
  theme(plot.title = element_text(size = 10))

ggplotly(ssm4, tooltip = "none", source = "none")
```

Trained Model {data-icon="ion-options-outline}
=====================================

Overview
--------------------------------------------------

### Chart A

WIP

Column 2
--------------------------------------------------

### 

```{r trained_model_1}

```

### 

```{r trained_model_2}

```

Conclusion {data-icon="ion-document"}
=====================================

Column 1
--------------------------------------------------

### Conclusion

Based on the insights gained from the feature analysis, it appears that there are clear patterns in the musical features that I find appealing and unappealing in songs. This raises the possibility of developing a classification model to predict whether or not I would like a particular song based on its acoustic features.

For example, a decision tree model could be trained on a dataset of songs that I have rated as either liked or disliked, using features such as loudness, energy, valence, and mode as predictors. The resulting model could then be used to predict the likelihood of me liking a new song based on its acoustic features.

While the plots provide some valuable insights into the musical features that I find appealing or unappealing, it's important to note that these are just a few of the many features that could potentially influence my musical taste. A more accurate classification model would need to take into account a broader range of features, such as tempo, rhythm, instrumentation, and genre, among others. Additionally, the model would need to be trained on a larger and more diverse set of songs to ensure that it can accurately classify songs that I like or dislike across a wider range of styles and genres. Nevertheless, the insights gained from these plots provide a good starting point for developing a more comprehensive model of my musical taste.